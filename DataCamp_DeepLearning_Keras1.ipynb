{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding your data\n",
    "\n",
    "You will soon start building models in Keras to predict wages based on various professional and demographic factors. Before you start building a model, it's good to understand your data by performing some exploratory analysis.\n",
    "\n",
    "The data is pre-loaded into a pandas DataFrame called df. Use the .head() and .describe() methods in the IPython Shell for a quick overview of the DataFrame.\n",
    "\n",
    "The target variable you'll be predicting is wage_per_hour. Some of the predictor variables are binary indicators, where a value of 1 represents True, and 0 represents False.\n",
    "\n",
    "Of the 9 predictor variables in the DataFrame, how many are binary indicators? The min and max values as shown by .describe() will be informative here. How many binary indicator predictors are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas  as pd \n",
    "df=pd.read_csv('/home/abderrazak/ALLINHERE/NLP/Datacamp/hourly_wages.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "      <td>534.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.024064</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>13.018727</td>\n",
       "      <td>17.822097</td>\n",
       "      <td>36.833333</td>\n",
       "      <td>0.458801</td>\n",
       "      <td>0.655431</td>\n",
       "      <td>0.292135</td>\n",
       "      <td>0.185393</td>\n",
       "      <td>0.044944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.139097</td>\n",
       "      <td>0.384360</td>\n",
       "      <td>2.615373</td>\n",
       "      <td>12.379710</td>\n",
       "      <td>11.726573</td>\n",
       "      <td>0.498767</td>\n",
       "      <td>0.475673</td>\n",
       "      <td>0.455170</td>\n",
       "      <td>0.388981</td>\n",
       "      <td>0.207375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.780000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>44.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       wage_per_hour       union  education_yrs  experience_yrs         age  \\\n",
       "count     534.000000  534.000000     534.000000      534.000000  534.000000   \n",
       "mean        9.024064    0.179775      13.018727       17.822097   36.833333   \n",
       "std         5.139097    0.384360       2.615373       12.379710   11.726573   \n",
       "min         1.000000    0.000000       2.000000        0.000000   18.000000   \n",
       "25%         5.250000    0.000000      12.000000        8.000000   28.000000   \n",
       "50%         7.780000    0.000000      12.000000       15.000000   35.000000   \n",
       "75%        11.250000    0.000000      15.000000       26.000000   44.000000   \n",
       "max        44.500000    1.000000      18.000000       55.000000   64.000000   \n",
       "\n",
       "           female        marr       south  manufacturing  construction  \n",
       "count  534.000000  534.000000  534.000000     534.000000    534.000000  \n",
       "mean     0.458801    0.655431    0.292135       0.185393      0.044944  \n",
       "std      0.498767    0.475673    0.455170       0.388981      0.207375  \n",
       "min      0.000000    0.000000    0.000000       0.000000      0.000000  \n",
       "25%      0.000000    0.000000    0.000000       0.000000      0.000000  \n",
       "50%      0.000000    1.000000    0.000000       0.000000      0.000000  \n",
       "75%      1.000000    1.000000    1.000000       0.000000      0.000000  \n",
       "max      1.000000    1.000000    1.000000       1.000000      1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying a model\n",
    "\n",
    "Now you'll get to work with your first model in Keras, and will immediately be able to run more complex neural network models on larger datasets compared to the first two chapters.\n",
    "\n",
    "To start, you'll take the skeleton of a neural network and add a hidden layer and an output layer. You'll then fit that model and see Keras do the optimization so your model continually gets better.\n",
    "\n",
    "As a start, you'll predict workers wages based on characteristics like their industry, education and level of experience. You can find the dataset in a pandas dataframe called df. For convenience, everything in df except for the target has been converted to a NumPy matrix called predictors. The target, wage_per_hour, is available as a NumPy matrix called target.\n",
    "\n",
    "For all exercises in this chapter, we've imported the Sequential model constructor, the Dense layer constructor, and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wage_per_hour', 'union', 'education_yrs', 'experience_yrs', 'age',\n",
       "       'female', 'marr', 'south', 'manufacturing', 'construction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predcitors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534, 9)\n"
     ]
    }
   ],
   "source": [
    "predictors=df.drop('wage_per_hour', axis=1)\n",
    "predictors=predictors.to_numpy()\n",
    "print(predictors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(534,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=df.iloc[:,1:]\n",
    "b=df.iloc[:,1:].values\n",
    "\n",
    "target =df.wage_per_hour.to_numpy()\n",
    "print(type(target))\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model\n",
    "\n",
    "You're now going to compile the model you specified earlier. To compile the model, you need to specify the optimizer and loss function to use. In the video, Dan mentioned that the Adam optimizer is an excellent choice. You can read more about it as well as other keras optimizers here, and if you are really curious to learn more, you can read the original paper that introduced the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "# tensor-Keras\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "\n",
    "# Set up the model: model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(50, activation='relu' ,input_shape=(n_cols,)))\n",
    "\n",
    "# Add the second layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(1))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "# Verify that model contains information from compiling\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model\n",
    "\n",
    "You're at the most fun part. You'll now fit the model. Recall that the data to be used as predictive features is loaded in a NumPy matrix called predictors and the data to be predicted is stored in a NumPy matrix called target. Your model is pre-written and it has been compiled with the code from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 611us/step - loss: 181.7273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4868490fd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(predictors,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_was_missing</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age  sibsp  parch     fare  male  age_was_missing  \\\n",
       "0         0       3  22.0      1      0   7.2500     1            False   \n",
       "1         1       1  38.0      1      0  71.2833     0            False   \n",
       "2         1       3  26.0      0      0   7.9250     0            False   \n",
       "3         1       1  35.0      1      0  53.1000     0            False   \n",
       "4         0       3  35.0      0      0   8.0500     1            False   \n",
       "\n",
       "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "0                        0                         0   \n",
       "1                        1                         0   \n",
       "2                        0                         0   \n",
       "3                        0                         0   \n",
       "4                        0                         0   \n",
       "\n",
       "   embarked_from_southampton  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('/home/abderrazak/ALLINHERE/NLP/Datacamp/titanic_all_numeric.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'pclass', 'age', 'sibsp', 'parch', 'fare', 'male',\n",
       "       'age_was_missing', 'embarked_from_cherbourg',\n",
       "       'embarked_from_queenstown', 'embarked_from_southampton'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_le (891,)\n",
      "target_oh (891, 2)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "import tensorflow as tf \n",
    "predictors=df.drop('survived', axis=1).to_numpy()\n",
    "n_cols = predictors.shape[1]\n",
    "target = df['survived'].to_numpy()\n",
    "#### Create the encoder.\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(target)   # Assume for simplicity all features are categorical.\n",
    "\n",
    "target_le = encoder.transform(target)\n",
    "\n",
    "target_oh =tf.keras.utils.to_categorical(target_le)\n",
    "\n",
    "print(\"target_le\",target_le.shape)\n",
    "\n",
    "print(\"target_oh\",target_oh.shape)\n",
    "\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictors = np.asarray(predictors).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 471us/step - loss: 2.4029 - accuracy: 0.5982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f47d6d44dd8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Convert the target to categorical: target\n",
    "\n",
    "# Set up the model\n",
    "model =Sequential()\n",
    "\n",
    "# Add the first layer\n",
    "model.add(Dense(32,activation='relu',input_shape=(n_cols,)))\n",
    "# Add the output layer\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(predictors,target_oh )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "The trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as pred_data. Use model to make predictions on your new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions: predictions\n",
    "predictions = model.predict(pred_data)\n",
    "# Calculate predicted probability of survival: predicted_prob_true\n",
    "predicted_prob_true = predictions[:,1]\n",
    "# print predicted_prob_true\n",
    "print(predicted_prob_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing optimization parameters\n",
    "\n",
    "It's time to get your hands dirty with optimization. You'll now try optimizing a model at a very low learning rate, a very high learning rate, and a \"just right\" learning rate. You'll want to look at the results after running this exercise, remembering that a low value for the loss function is good.\n",
    "\n",
    "For these exercises, we've pre-loaded the predictors and target values from your previous classification models (predicting who would survive on the Titanic). You'll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function get_new_model() that creates an unoptimized model to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SGD optimizer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [.000001, 0.01,1]\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_new_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer =SGD(lr=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=my_optimizer )\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(predictors,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model accuracy on validation dataset\n",
    "\n",
    "Now it's your turn to monitor model accuracy with a validation data set. A model definition has been provided as model. Your job is to add the code to compile it and then fit it. You'll check the validation score in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 1.4484 - accuracy: 0.5955 - val_loss: 0.5690 - val_accuracy: 0.7388\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7808 - accuracy: 0.6501 - val_loss: 0.6133 - val_accuracy: 0.7127\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6130 - accuracy: 0.6854 - val_loss: 0.5100 - val_accuracy: 0.7649\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6084 - accuracy: 0.6774 - val_loss: 0.5880 - val_accuracy: 0.7425\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6498 - accuracy: 0.6918 - val_loss: 0.6565 - val_accuracy: 0.6754\n"
     ]
    }
   ],
   "source": [
    "# Import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Save the number of columns in predictors: n_cols\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Specify the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor =  EarlyStopping( patience=2)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(predictors,target_oh ,validation_split=0.3,epochs=30,callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEJCAYAAAB7UTvrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2EElEQVR4nO3deXhTdd7+8XfSdN+bdIFCBdqURVktolVZO8CAQEcUFHEeBR8XFBwXRkR+oqMd64IsKm5UcHTmEVFx3BAsgyAtKosoitiWbUQKbdN9b3ry+yM2UNpCKM1yms/rurxskpPkbq5y7pxzvud7NBaLxYIQQggBaF0dQAghhPuQUhBCCGEjpSCEEMJGSkEIIYSNlIIQQggbKQUhhBA2OlcHuFDHjx9v1/MMBgNFRUUdnMZx1JRXTVlBXXnVlBXUlVdNWeHC8nbt2rXNx2RLQQghhI2UghBCCBspBSGEEDZSCkIIIWykFIQQQthIKQghhLCRUhBCCGHjkaXw8886Fi70oqJC4+ooQgjhVjyyFH791YslS7zIyVH9uXtCCNGhnLJWXLlyJXv27CE0NJQlS5a0eNxisbB69Wq+++47fH19mTNnDr169XJYHqPRDEBeno5LL21w2PsIIYTaOGVLYeTIkSxcuLDNx7/77jtOnDjBihUruP3221m1apVD88TFNeLrayE319uh7yOEEGrjlFLo168fQUFBbT6+a9cuhg8fjkajITExkaqqKkpKShyWx8sLjEaL7D4SQogzuMVasbi4GIPBYLut1+spLi4mPDy8xbKZmZlkZmYCkJ6e3ux556NvXw27dvm2+/nOptPpJKuDqCmvmrKCuvKqKSs4Lq9blILFYmlxn0bT+siglJQUUlJSbLfbO0tg797RvPeell9/LcLfv10v4VRqmsFRTVlBXXnVlBXUlVdNWaGTz5Kq1+ub/XImk6nVrYSO1LevBYtFw6FDbtGLQgjhFtyiFJKSkti2bRsWi4WcnBwCAgIcXgp9+li3TvLypBSEEKKJU9aIy5YtY//+/VRUVHDnnXcybdo0zGbrsNCxY8cyePBg9uzZw7x58/Dx8WHOnDkOz2Q0WtBqLeTkeAO1Dn8/IYRQA6eUwl/+8pezPq7RaLjtttucEcXG19c6NDU3V7YUhBCiiVvsPnKVxMQG2X0khBCn8ehSMBrNHDqk4/c9WUII4fE8uhQSEsw0NGg4csTL1VGEEMIteHQpnJoDSaa7EEII8PBSSEiwloIcbBZCCCuPLoXgYAtdusgIJCGEaOLRpQBgNMoIJCGEaCKlYDSTm6tDUVydRAghXM/jSyEhwUx1tZb8fBmBJIQQHl8KTSOQ5LiCEEJIKZCYKKUghBBNPL4U9HqF8HAZgSSEECClAJw62CyEEJ5OSgEpBSGEaCKlgLUUSkq8MJnk4xBCeDZZCyIjkIQQoomUAqdKISdHSkEI4dmkFICuXRsJCFBkugshhMeTUgA0GuuZzbL7SAjh6aQUfmcdgSTXVRBCeDYphd8ZjWby872oqNC4OooQQriM0/aX7N27l9WrV6MoCmPGjCE1NbXZ45WVlbz88sucPHkSb29v7rrrLuLi4pwV77SrsOkYPLjBae8rhBDuxClbCoqikJGRwcKFC1m6dClZWVkcO3as2TLr16+nR48ePPfcc9xzzz2sWbPGGdFsEhKsRSDHFYQQnswppZCXl0dMTAzR0dHodDqSk5PZuXNns2WOHTtG//79AYiNjaWwsJDS0lJnxAOgR49GvL0tMgJJCOHRnLIGLC4uRq/X227r9Xpyc3ObLXPRRRfxzTff0KdPH/Ly8igsLKS4uJiwsLBmy2VmZpKZmQlAeno6BoOhXZl0Ol2L5xqNFo4eDcRg8G3XazpSa3ndlZqygrryqikrqCuvmrKC4/I6pRQsFkuL+zSa5gd0U1NTWbNmDfPnzycuLo6ePXui1bbckElJSSElJcV2u6ioqF2ZDAZDi+f27BnOjz96t/s1Ham1vO5KTVlBXXnVlBXUlVdNWeHC8nbt2rXNx5xSCnq9HpPJZLttMpkIDw9vtkxAQABz5swBrCVyzz33EBUV5Yx4NkajmQ0b/KitBT8/p761EEK4BaccU4iPjyc/P5+CggLMZjPZ2dkkJSU1W6aqqgqz2ToCaPPmzfTt25eAgABnxLMxGs0oiobDh+W4ghDCMzll7efl5cWsWbNIS0tDURRGjRpF9+7d2bRpEwBjx47lt99+48UXX0Sr1dKtWzfuvPNOZ0Rrxmg8NQKpb1+z099fCCFczWlfiYcMGcKQIUOa3Td27Fjbz4mJiaxYscJZcVrVq5cZjUZGIAkhPJec0Xwaf3+Ii2skJ0emuxBCeCYphTMkJJhlS0EI4bGkFM5gNJo5dEhHY6OrkwghhPNJKZwhMbGBujoN//2vl6ujCCGE00kpnCEhQS7NKYTwXFIKZzh1vWY52CyE8DxSCmcICbEQHd0oWwpCCI8kpdAKo1FGIAkhPJOUQiuMxgZyc3W0Mo+fEEJ0alIKrUhIMFNZqSU/Xz4eIYRnkbVeK05dmlMONgshPIuUQitOjUCS4wpCCM8ipdCKyEiFsDBFSkEI4XHOqxSKiorIyclxVBa3odHIHEhCCM9k11qvqKiI5cuXc+TIEQDeeustvv76a/bu3euS6x44g9HYwKZNcvk1IYRnsWtL4bXXXmPw4MG8+eab6HTWHhkwYAA//PCDQ8O5UkKCGZPJi+JizbkXFkKITsKuUsjLyyM1NRWt9tTiAQEBVFdXOyyYq8kIJCGEJ7KrFEJDQzlx4kSz+44dO4bBYHBIKHeQmCgjkIQQnseuNd6kSZN4+umnSU1NRVEUtm/fzvr160lNTXVwPNeJjW3E318hJ0dKQQjhOexa440ePZqgoCA2b96MXq9n27ZtTJ8+ncsuu8zR+VxGq4X4eBmBJITwLOdc4ymKwt/+9jceeeSRTl0CrTEazXz7rY+rYwghhNOcsxS0Wi0FBQVYLnB2uL1797J69WoURWHMmDEtdj1VV1ezYsUKTCYTjY2NTJo0iVGjRl3Qe14oo9HM+vUBVFVpCAyU2fGEEJ2fXQear7vuOl5//XUKCwtRFKXZf/ZQFIWMjAwWLlzI0qVLycrK4tixY82W+fzzz+nWrRvPPvssjz32GP/4xz8wm83n/xt1oKYRSAcPyi4kIYRnsGtt9+qrrwKwbdu2Fo+tXbv2nM/Py8sjJiaG6OhoAJKTk9m5cyfdunWzLaPRaKitrcVisVBbW0tQUFCzIbCu0FQKOTk6BgxocGkWIYRwBrtK4cUXX7ygNykuLkav19tu6/V6cnNzmy0zfvx4nnnmGe644w5qamq47777XF4KPXqY0eksMixVCOEx7FrbRUZGAtbdQGVlZYSGhp7XCru14xEaTfMzhb///nsuuugiHn30UU6ePMkTTzxBnz59CAgIaLZcZmYmmZmZAKSnp7f7XAmdTmfXc+Pj4b//DcRg8G3X+3QUe/O6AzVlBXXlVVNWUFdeNWUFx+W1qxSqq6t54403yMrKQlEUvLy8SE5OZtasWS1W2q3R6/WYTCbbbZPJRHh4eLNltmzZQmpqKhqNhpiYGKKiojh+/DgJCQnNlktJSSElJcV2u6ioyJ5foQWDwWDXc+Pjw/npJ12736ej2JvXHagpK6grr5qygrryqikrXFjerl27tvmYXV/3V69eTW1tLUuWLOHtt9/mueeeo76+njfeeMOuAPHx8eTn51NQUIDZbCY7O5ukpKRmyxgMBvbt2wdAaWkpx48fJyoqyq7Xd6SEBDNHjuior3d1EiGEcDy7thT27t3Liy++iK+vdRdK165dmTNnDnPnzrXrTby8vJg1axZpaWkoisKoUaPo3r07mzZtAmDs2LFMnTqVlStX8sADDwBw0003ERIS0p7fqUMZjWYaGzUcPqyjd2/XjoYSQghHs6sUfHx8KC8vtx1bACgvL7fNmGqPIUOGMGTIkGb3jR071vZzREQEixYtsvv1nOX0q7BJKQghOju7p7l48sknmThxIpGRkRQWFvLpp58227ffWSUkmNFoZASSEMIz2LWmu/baawkPDycrK4vi4mIiIiKYMmWKy884dgZ/fwvdujXKHEhCCI9g15pOo9EwevRoRo8e7eg8bsloNJOTI9dVEEJ0fnaNPnrjjTf45Zdfmt33yy+/sGbNGkdkcjsJCWYOHdLR2OjqJEII4Vh2lUJWVhbx8fHN7uvVqxfbt293SCh3YzSaqa3VcOyYl6ujCCGEQ9lVChqNpsXkd4qiXPDMqWphNFrnPZKDzUKIzs6uUujTpw/vvPOOrRgURWHdunX06dPHoeHcxanrNUspCCE6N7vWcrfeeivp6enccccdtlOrw8PDeeihhxydzy2EhVmIjGyUg81CiE7PrlLQ6/U8/fTT5OXlYTKZ0Ov1JCQkuHwWU2dKSDDL7iMhRKdn91pdq9WSmJjIFVdcQX19PQcOHHBkLrdjNFqv1+whh1GEEB7KrlJYvHixrQQ+/PBDli9fzvLly/nggw8cGs6dJCY2UF6upaDAc7aOhBCex6413K+//kpiYiIAmzdvZvHixaSlpfHFF184NJw7SUg4dRU2IYTorOwqhaahpydOnACgW7duGAwGqqqqHJfMzcgIJCGEJ7BrDde7d2/eeOMNSkpKGDp0KGAtiODgYIeGcyfR0QrBwQq5uTICSQjRedm1pXD33XcTEBDARRddxLRp0wA4fvw4EyZMcGg4d6LRWLcWZASSEKIzs2sNFxwczIwZM5rdd+a1ETyB0WhmyxbXXqtZCCEcSYbSnAejsYGCAi9KSzWujiKEEA4hpXAemkYgyS4kIURnJaVwHk6NQJKDzUKIzklK4Tx0796In59cmlMI0XnZtXarrKzko48+4ujRo9TW1jZ77PHHH3dIMHfk5QW9eskIJCFE52XX2m358uWYzWauuOIKfHx8HJ3JrRmNDezZ49mfgRCi87KrFHJycli1ahXe3u3fl753715Wr16NoiiMGTOG1NTUZo9/9NFHfPXVV4D1eg3Hjh0jIyODoKCgdr+nIxiNZj76yJ+aGg3+/jI7nhCic7GrFOLi4jCZTMTExLTrTRRFISMjg0WLFqHX63n44YdJSkqiW7dutmUmT57M5MmTAdi1axeffvqp2xUCWEvBYtFw8KAXl1xidnUcIYToUHaVwiWXXMLf//53Ro4cSVhYWLPHRo8efc7n5+XlERMTQ3R0NADJycns3LmzWSmcLisriyuvvNKeaE7XNAIpN9dbSkEI0enYVQoHDhxAr9ezb9++Fo/ZUwrFxcXo9Xrbbb1eT25ubqvL1tXVsXfvXmbPnt3q45mZmWRmZgKQnp6OwWCw51doQafTteu5ISHg5WXh2LEQDIbAdr13e7Q3ryuoKSuoK6+asoK68qopKzgur12lsHjx4gt6E0srV6bRaFo/K3j37t307t27zV1HKSkppKSk2G4XFRW1K1PTZUXb46KLovjhhwaKikra9fz2uJC8zqamrKCuvGrKCurKq6ascGF5u3bt2uZjdo+trKysZPfu3RQXFxMREcGll15q9z5/vV6PyWSy3TaZTISHh7e6bFZWFldddZW9sVzCaGyQYalCiE7JrpPXcnJymDt3Ll988QVHjx4lMzOTuXPnkpOTY9ebxMfHk5+fT0FBAWazmezsbJKSklosV11dzf79+1t9zJ0YjWYOH9bR0ODqJEII0bHs+rq7Zs0abrvttmYHf7Ozs1m9ejVPPfXUOZ/v5eXFrFmzSEtLQ1EURo0aRffu3dm0aRMAY8eOBeDbb79l4MCB+Pn5ted3cRqj0YzZrOHoUZ1tPiQhhOgM7CqF/Px8rrjiimb3XX755bz++ut2v9GQIUNaTLfdVAZNRo4cyciRI+1+TVdpGoGUkyOlIIToXOzafRQTE0N2dnaz+3bs2GEbYuppZLZUIURnZdda7ZZbbiE9PZ0NGzZgMBgoLCwkPz+fBQsWODqfWwoMtNC1q1mu1yyE6HTsvkbzCy+8wJ49eygpKeHSSy9lyJAhbnnGsbMkJsrEeEKIzsfutVpQUBDDhw93ZBZVSUgw8803ASgKaGUCciFEJ9FmKaSlpfHII48A8Oijj7Z5spknTZ19OqPRTE2Nlt9+86J790ZXxxFCiA7RZimMGDHC9rM9U1l4mlNzIOmkFIQQnUabpXD6WcWxsbEYjcYWy+Tl5TkmlQoYjdYz13JzdYweXefiNEII0THs2hv+5JNPtnp/Wlpah4ZRk4gIC3p9o4xAEkJ0KmddoymKAlgntGv6r8nJkyfx8vJybDo3ZzSayclp/4WHhBDC3Zy1FG688UbbzzfccEOzx7RaLX/6058ck0olEhLMfPKJPxYLtHEcXgghVOWspfDiiy9isVh47LHHmo0y0mg0hISEyPWajWZKS7UUFWmJjFRcHUcIIS7YWUshMjISgJUrVzoljNokJp4agRQZWe/iNEIIceHsPkq6a9cu9u/fT3l5ebP777nnng4PpRYJCadGICUnSykIIdTPrtFH69at47XXXkNRFL7++muCgoL4/vvvCQgIcHQ+t9ali0JQkCLTXQghOg271mZbtmxh0aJFxMXF8eWXX3LLLbdw1VVX8f777zs6n1vTaKwHm3NzZQSSEKJzsGtLoaqqiri4OMB6sWiz2UxCQgL79+93aDg1SEiQ2VKFEJ2H3ddT+PXXXwFsV0zbtm2bR8+S2iQx0cyJE16Ul8uYVCGE+tn1FXf69OlUVFQAMGPGDFasWEFtbS233XabQ8OpwenTXVx6qVy0WQihbnaVwumX0TQajbzwwgsOC6Q2TVdhy8uTUhBCqF+bpXDy5Em7XsBTL8nZJC6uER8fy+8Hm2tcHUcIIS5Im6Uwb948u15g7dq1HRZGjXQ6iI+Xq7AJITqHNtdkp6/st2zZwr59+7j++uuJjIyksLCQ9957j/79+9v9Rnv37mX16tUoisKYMWNITU1tscxPP/3EmjVraGxsJDg4WDUX8ElIMLNvnwxLFUKon11fb9euXcuKFStscx116dKF22+/nXvvvZeRI0ee8/mKopCRkcGiRYvQ6/U8/PDDJCUl0a1bN9syVVVVrFq1ikceeQSDwUBZWVn7fiMXMBrNfPKJHzU14O/v6jRCCNF+dg1JtVgsFBQUNLuvsLDQNrX2ueTl5RETE0N0dDQ6nY7k5GR27tzZbJnt27czbNgwDAYDAKGhoXa9tjtISGjAYtFw6JDsQhJCqJtda7GJEyfyt7/9jZEjR2IwGCgqKmLr1q1MnDjRrjcpLi5Gr9fbbuv1enJzc5stk5+fj9ls5rHHHqOmpoYJEyY0uyRok8zMTDIzMwFIT0+3lcj50ul07X7umS67zHqOwsmTEYwY4ZjZUjsyr6OpKSuoK6+asoK68qopKzgur12lMHnyZOLi4tixYwdHjhwhLCyMu+66i0GDBtn1JqdfnKeJ5owLEDQ2NnL48GH+3//7f9TX17No0SKMRiNdu3ZttlxKSgopKSm220VFRXZlOFNTuXWE8HDQaruwZ08No0dXdMhrnqkj8zqamrKCuvKqKSuoK6+assKF5T1zvXo6u/d3DBo0yO4SOJNer8dkMtlum0wmwsPDWywTHByMn58ffn5+9O3bl6NHj541vLvw87MOTZURSEIItWtzLfbBBx9w7bXXAmcfdjp9+vRzvkl8fDz5+fkUFBQQERFBdnZ2iyGvSUlJvPHGGzQ2NmI2m8nLy7N795Q7MBplWKoQQv3aXIud+c3+Qnh5eTFr1izS0tJQFIVRo0bZ5lACGDt2LN26dWPQoEE8+OCDaLVaRo8ebZuETw2Mxga+/NIXs9l67oIQQqiRxtLaDn8VOX78eLue19H7D99915/77gtn27aTxMc3dtjrNlHT/k41ZQV15VVTVlBXXjVlBRccU5BpLs6P0dg0B5K3Q0pBCCGcQaa56CBNE+Pl5OgYN87FYYQQop3smuZCnFtwsIWYGHWNQKqq0hAQYEEjl4IQQvzOrjOahX2MRjM//ODN8ePu+7H+9puWV18N5JprDCQmdmHKFAPbt/u4OpYQwk3Y9bW2sbGRjRs3sn//ftvFdpqoZdI6Z7jiijqeeSaEoUNjGDiwnrFjaxk3rpY+fcwu/TZ+8qSWTz7x5+OP/di50xeASy6p5447Kvn3v/2ZPt3AlVfW8de/lpOUJNeEEMKT2fWV9s033yQzM5N+/fpx6NAhhg0bRllZGRdffLGj86nKvHmVbNlSwIIF5Wi18OyzIaSkRJGcHMXixSFkZ/tgNjsnS2GhljVrArjuOj2XXhrNo4+GUlmp5a9/LWfbtpNs3FjEo4+Wk5V1kscfL+OXX3RMmRLJzTdH8OOP6tkFJoToWHb96//mm29IS0vDYDDw7rvvMmHCBAYOHMhrr73m6HyqotFYr9mcmFjJ3LmVnDyp5Ysv/Ni40Y+33gpk1aogwsIUxoypZfz4WkaMqCMwsONGBBcXa/nsMz8+/tif7GwfFEWD0djA/fdXMGlSrW2E1On8/OC226q48cZqVq8O5OWXgxg3LooJE2qYP7+CxEQntZgQwi3YVQr19fW2Ce18fHyoq6sjNjaWI0eOODKb6kVHK8ycWc3MmdVUVWn48ktfNm70Y/NmP95/PwBfXwtXXVXHuHG1/OEPtURFnf9keqWlGj7/3FoEX33lS2Ojhp49zcydW8nkyTX07m3frqvAQAv33FPJzTdX8frrQbz2WiAbNvjxpz/VcP/9FfTsKcNshfAEdpVCbGwsBw8eJCEhgV69erFu3Tr8/f2JiIhwdL5OIzDQwsSJtUycWIvZDN9848PGjX5s2uTH5s1haDQWBg9uYNw463GIhIS2V+bl5Ro2brQWwbZtvjQ0aIiLM3PXXZVMmlTDxRe3/xhGaKiFBx+sYNasKlauDGL16gD+/W9/brihmnvvrSQ2VspBiM7srGc0K4qCVqslLy8PrVZLr169yM/PZ9WqVdTU1HDzzTfTt29fZ+ZtwV3OaG4viwV+/llnK4gffrCOBOrZ08y4cdbdTEOG1BMQYOCdd6r4+GM/tmzxo75eQ2ysmUmTapk8uYYBAxoccjD75EktL7wQxNtvB6LRwMyZVcydW3nWrRp3+Wztpaa8asoK6sqrpqzguDOaz1oKt99+O8OHD2f48OFuOw+R2kvhTMePa9m0yVoQ2dnWrYCIiEaqq7XU1mqIiWnkmmtqmDy5hiFDHFMErTl2zIvly4NYuzYAb28Ls2ZVcdddlUREtPzzcdfPti1qyqumrKCuvGrKCi4qhZ07d/LVV1+xe/duunXrxogRI7jqqqsICQlpVxBH6GylcLrycg1btviyebMf0dG+/OEPJSQl1aN14WkQhw55sXRpMOvX+xMYaOH226v43/+tJCTk1J+RGj7b06kpr5qygrryqikruKgUmlRVVZGdnc22bds4dOgQAwYMYMSIESQlJaFz8ZSgnbkUTudueQ8c0LFkSTCffeZPWJjCnDmV3HprFQEBFrfLei5qyqumrKCuvGrKCi4uhdMVFBSwbds2Nm/eTH19PRkZGe0K1VGkFFzrhx+8efbZYP7zHz8iIxuZO7eSe+/1p7LS/bK2xV0/29aoKSuoK6+asoIbXHkNoKGhgby8PHJzcykrK6N3797tCiQ6jwEDGnjrrWJ27vTh6aeDefTRUJYvtxAXZ0CvVzAYGjEYlNP+O3U7IkLBy8vVv4EQ4nR2lcKBAwfYunUrO3bsIDQ0lKuvvprbbruNyMhIR+cTKjF0aD3r1pnYvt2HTz8N59dfFfLzvfjxR2+KirSYzS2PiGs0FiIiWi+M1m4HBKj60h9CqMJZS+Hdd9/lq6++orKykssvv5wFCxbQp08fZ2UTKqPRwNVX1/OnPzVSVFRsu99isZ5kZzJ5UVSkpbBQi8mkpajI67SftXz/vQ8mk5aKitaPpPv5KYSFWQgLU2z/hYZaTvvZ+v/wcIvt57AwheBgi0sPzguhJmcthdzcXG644QaGDh2Kj4/MpCnaR6OB8HAL4eFmEhLOvXxtLbYCOfWfFyaTlrIyDaWlWsrKtBw9qqOkxHpfTU3ba32t1kJISPMyOb1Qpk/XcNFFHfgLC6FiZy2FRx55xFk5hLDx84PY2MbzOnu6thbKyqxlUVqqpbRU8/v/m9/X9PORIzrKyqy3X3wRHnggiHvuqZRjHMLjyXSYolPw87PuXoqOPr/5o8rLNSxeHMUzz4SwbZsvK1aUylQewqPJnlbh0UJCLPzjH40sW1bCvn3e/OEPkXz8sZ+rYwnhMlIKwuNpNHD99TVs2lRIr15m7rwzggceCKWqSq5TKtzTqlWB5OQ45rWdtvto7969rF69GkVRGDNmDKmpqc0e/+mnn3jmmWeIiooCYNiwYVx33XXOiicEPXo0sn59Ec8/H8wLLwTxzTe+vPRSCQMHytXohPtYtiyIZ58NwWRq5KGHOv71nVIKiqKQkZHBokWL0Ov1PPzwwyQlJdGtW7dmy/Xt25cFCxY4I5IQrfL2hoceqmD48Drmzg1n8mQDDz1UwZ13VsqwVuFyy5dbC2Hq1GrS03WUlHT8ezjlzzwvL4+YmBiio6PR6XQkJyezc+dOZ7y1EO1yxRX1fPFFAePG1ZKWFsINN+jJz5dWEK6zfHkQzzxjLYSlS0sdNlLOKVsKxcXFtiu3Aej1enJzc1ssl5OTw/z58wkPD+fmm2+me/fuLZbJzMwkMzMTgPT0dAwGQ7sy6XS6dj/XFdSUV01Zoe28BgO8/z6sWWPm/vt9GDs2mldeMTNliuvOrO4sn607cues6elannlGx4wZjaxapcPLy+CwvE4phdbm3NOccSGAnj17snLlSvz8/NizZw/PPvssK1asaPG8lJQUUlJSbLfbOyGUJ01+5WxqygrnzjtpEvTt68U994QzbZoPM2dW8dhj5fj7O78c1PLZ1tXBl1/6kZQUhF7v/nnBfT/bFSuCePrpEK69tpr09FLbLiNHTYjnlO1hvV6PyWSy3TaZTISHhzdbJiAgAD8/61DAIUOG0NjYSHl5uTPiCXFOCQmNfPRREXPmVPD224GMH2/gxx/lNJ8zFRRoee65YC67LJpZsyJISvJmxYogzGZXJ1On0wth2TLH7TI6nVNKIT4+nvz8fAoKCjCbzWRnZ5OUlNRsmdLSUtsWRV5eHoqiEBwc7Ix4QtjFxwceeaSC//u/IioqtEyaFMlrrwWinN/5cp3S3r3ezJ0bxmWXRbNsWRCDBjWwerWJyZMVnn46hGuuMbB/v5To+XBFIYCTdh95eXkxa9Ys0tLSUBSFUaNG0b17dzZt2gTA2LFj+frrr9m0aRNeXl74+Pjwl7/8pcUuJiHcwfDh9WRmFvLgg6E8/ngoW7f6snRp6VmvW90eNTUafv5Zx/793uzf781PP3lTVubNlVeGMn58DZdfXo+3d4e+5XlpaIDPPvMjIyOI3bt9CApS+POfq7j11ip69rSeFT5jRiNjx5axcGEoEyZEcu+9FdxzT6VLc6vBCy+4phCgHRfZcTdykR33o6as0P68Fgu89VYAjz8eSmCgwvPPl5KSUteu1zlxQstPP3nbCmD/fh2HDumwWKxfjIKDFfr1ayAiwpstW6C2VktYmEJKSi1//GMtI0bUOe0YR3Gxln/+M4A1awI5ccKLHj3MzJpVxbRp1QQHN8/Q9NkWF2t59NEQ1q8P4OKLG3j++RIuucS99im5y9/tCy8EkZ4ewp/+VM3y5W0Xgttcec3dSCm4HzVlhQvPm5OjY86ccH7+2ZtZsyp55JFy/NqYKaO+HnJzm3/7379fR0nJqX/5cXFm+vVroF8/Mxdf3EC/fg10796IRmPN+uuvJrZu9WXDBj8yM/0oLdXi56cwalQd48fXkpJSS1hYx/+z/vlnHRkZgaxfH0BtrYarr65j9uxKxoypa/McjjM/240b/ViwIJTiYi1z51Yyb14F7jIBszv83dpbCCCl0CYpBfejpqzQMXlra+Gpp0JYtSqIPn0aeOmlEqKiGk9b8Vv/y83V0dBg/fbv52ehd+8G24q/Xz8zffs2EBLS9j/JM7M2NMDXX/vw+ef+fP65HydOeKHTWbjiinrGj69h3LhaunRp/26txkb44gs/MjICyc72xc9P4brrapg9u4rExHN/02/tsy0p0bB4cSjvvx9A374NPP98KQMGuP6scVf/3b74YhBPPWVfIYCUQpukFNyPmrJCx+bdssWXv/wljOJiLYpy6phYVFRjs5X/xRc30LOnGd15HtU7W1ZFge+/9+bzz/3YsMGPgwetO+4HD67nj3+sZfz4GuLj7ZsBtqxMwzvvWHcR/fe/Orp2NXPrrdXceGMV4eH2rzLOlveLL3xZsCCMwkItc+ZUct99Ffj62v3SHc6Vf7enF8KyZaV2/V1IKbRBSsH9qCkrdHzewkItq1YFEhamcPHF1l1BBkPHHIQ+n6y5uTo2bPDj88/9+P576z6axMQGxo+vZfz4WgYMaODMsRx5eV688UYQ69b5U12tZdiwOmbNqmL8+NrzLjB78paVaXj88VDWrg0gMdG61TB4sGu2Glz1d/vSS0H8/e8hpKZatxDs/ZylFNogpeB+1JQV1JW3vVl/+03Lxo3+bNjgxzff+NDYqKFrV7OtIOrrNWRkBLJlix8+PhamTLHuIurf/8JW0Pbm3bLFl/nzwzh5Usudd1bywAMVbR6X6UiKAvv368jO9qW0NIiRI0sYOrS+RVk6SnsLAaQU2iSl4H7UlBXUlbcjshYXa/jiC+sWxLZtftTWWteAUVGN/PnPVcycWU1kpPO3bMrLNTzxRAj/+lcgCQkNLFlSSlJSx241WCyQl6cjK8uHrCzf38vAepTc29tCQ4OGuDgzU6fWMHVqtW1orSNcSCGAlEKbpBTcj5qygrrydnTWqioNW7f60tgI48bVdvhIoPbk3bbNlwcfDOX4cS9uv72K+fPL8fdv3/tbLHD0qBfZ2b5kZfmQne1LQYH1CG63bmauvLKeK6+sIzm5jp49I3j77Sref9+fr77yxWLRcOml9UydWs3kyTXndSzlXFauDCItLYQpU6pZseL8CwGkFNokpeB+1JQV1JVXTVmh/XkrKzU8+WQIb70VSM+eZp5/vpTLLqu367nHj2ttWwFZWT789pt1jRsd3Uhycp2tCOLimm8FnJ41P1/Lhx/6s25dAL/84o2Pj4WUlFqmTq1h9OgLK8+OKIQz854vKYVWeMo/LldQU1ZQV141ZYULz7t9uw8PPhjGsWNezJpVxYIFFQQENF9lFRZqyc4+tTvo8GHrWjY8vJHk5HqSk+u46qp64uPNZz1W0FpWiwV++knHe+8FsH69P0VFXoSFKUyZUsN111UzeHDLg/Vn01GF0FZee52tFGQyEiGE27rqqno2by7kqaeCycgIIjPTj7S0MurqNLbjAr/8Yh16GxyscPnl9fzP/1SRnFxH377mC74wkkYDl1xi5pJLylm0qJxt23x57z1/1q4N4M03A+nVy8zUqdVMnVpD9+5nP/7w8suBpKWFMHlyzQUXgiPJloJKqCmvmrKCuvKqKSt0bN4dO6xbDUeOWNem/v4Kl11Wb9sddMklDU775l1eruGzz/x4770Aduywnlxx+eV1XHddDRMn1rQ4AfHllwN58slQJk+u4YUXSjqkEGRLQQjh0axXwyvkk0/86NGjkUGD6l02RUZIiIUbbqjhhhtq+PVXLz74wJ/33/fnwQfDWLQolLFja5k6tZoRI+pYtarjC8GR3DyeEEKcEhBgYdq0GlfHaKZ790buvbeSefMq2bvXm/ff9+fDD/356CN/wsIUSku1TJqkjkIAKYUOpfv5Z7y//57aP/4RS2ioq+MIIZxIo4HBgxsYPLiBRx8tZ8sWP95/35+ICIUnnyxTRSGAlEKH8f7uO/Q33oi2ogLLI49QM3Ei1TNmUD9sGE47PVII4RZ8fKznfYwbV+vqKOfNKVde6+y89+xBf+ONKBERmN56i+rrr8dv40YMU6cSNXw4QStXoi0ocHVMIYQ4JymFC+S9ezf6GTNQIiIoWreOutGjKUtP5+R331GydCmNBgMhaWlEDx1K+OzZ+GZmIhesFUK4K9l9dAFshWAwUPTuuyixsbbHLAEB1EybRs20aejy8gh45x38330X/88/pzEmhurp06m+4QYa4+Jc+BsIIURzsqXQTt67dp0qhHXrmhXCmcwJCZQvWsTJXbsofv11Gvr1I+iFF4i+4gr006fj9+9/W6/SIoQQLiZbCu3gvXMn+pkzTxXCWU4EacbHh9oJE6idMAHtb78R8O67BLzzDhFz5qCEhVE9dSrVN96IuW9fx/4CQgjRBtlSOE/eO3eiv+kmayG89579hXAGJTaWyvvuo2DHDkz/93/UDR9O4FtvEZWSguGaawj45z/RVFZ2cHohhDg7p5XC3r17uffee5k7dy4ffvhhm8vl5eUxffp0vv76a2dFs5tPUyFERVkLoUuXC39RrZa64cMpefllTu7eTdljj6Gpribsr38levBgQh94AO9du6wzcwkhhIM5pRQURSEjI4OFCxeydOlSsrKyOHbsWKvL/fOf/2TQoEHOiHVefL79loimQli3rmMK4QxKRARV//u/FG7eTOHHH1OTmor/Rx8ROWUKXn/+c4e/nxBCnMkpxxTy8vKIiYkhOjoagOTkZHbu3Em3bt2aLbdhwwaGDRvGwYMHnRHLbrZCiImxFkJMjGPfUKOhYcgQyoYMoXzxYvw//pjAiy5y7HsKIQROKoXi4mL0er3ttl6vJzc3t8Uy3377LYsXL+bll19u87UyMzPJzMwEID09HYPB0K5MOp3Orudqtm9HN3MmxMaibNpERDuPIbSbwQBz5+Kl02FQyfkN9n627kJNedWUFdSVV01ZwXF5nVIKrc3OrTlj6oc1a9Zw0003oT3HBOgpKSmkpKTYbrd36lh7pp31+fprIm6+GXOXLpjWrkXx8QEXTVuspimT1ZQV1JVXTVlBXXnVlBVUPnW2Xq/HZDLZbptMJsLDw5stc/DgQZYvXw5AeXk53333HVqtlssuu8wZEVtoKoTGrl0xvfsuyu+7voQQojNzSinEx8eTn59PQUEBERERZGdnM2/evGbLvPTSS81+vvTSS11XCDt2WAshNhbTunUoUVEuySGEEM7mlFLw8vJi1qxZpKWloSgKo0aNonv37mzatAmAsWPHOiOGXXyys4n4859p7NbNuoUghSCE8CBOO6N5yJAhDBkypNl9bZXB3Xff7YxILTQrhHXrUCIjXZJDCCFcRc5o/p1PVpZ1l1H37lIIQgiPJaUA+Gzfbt1CiIuz7jKSQhBCeCiPLwWfr74i4n/+h8aLLpItBCGEx/PoUvD56iv0t9xCY48e1i0EFZ24IoQQjuCxpaDZvBn9Lbdg7tlTCkEIIX7nkaXgs307umuvtRbC2rUop03BIYQQnswjS0GJicFy1VXWLQQpBCGEsPHIUjAnJGD+9FOUiAhXRxFCCLfikaUghBCidVIKQgghbKQUhBBC2EgpCCGEsJFSEEIIYSOlIIQQwkZKQQghhI2UghBCCBuNxWKxuDqEEEII9+CxWwoLFixwdYTzoqa8asoK6sqrpqygrrxqygqOy+uxpSCEEKIlKQUhhBA2HlsKKSkpro5wXtSUV01ZQV151ZQV1JVXTVnBcXnlQLMQQggbj91SEEII0ZKUghBCCBudqwO4wt69e1m9ejWKojBmzBhSU1NdHalVRUVFvPTSS5SWlqLRaEhJSWHChAmujnVOiqKwYMECIiIi3HqYX1VVFa+88gq//vorGo2Gu+66i8TERFfHatMnn3zCf/7zHzQaDd27d2fOnDn4+Pi4OpbNypUr2bNnD6GhoSxZsgSAyspKli5dSmFhIZGRkdx3330EBQW5OGnrWd966y12796NTqcjOjqaOXPmEBgY6OKkVq3lbfLRRx/x9ttvs2rVKkJCQi74vTxuS0FRFDIyMli4cCFLly4lKyuLY8eOuTpWq7y8vLj55ptZunQpaWlpbNy40W2znu6zzz4jNjbW1THOafXq1QwaNIhly5bx7LPPunXm4uJiNmzYQHp6OkuWLEFRFLKzs10dq5mRI0eycOHCZvd9+OGH9O/fnxUrVtC/f38+/PBD14Q7Q2tZBwwYwJIlS3juuefo0qUL69evd1G6llrLC9Yvjvv27cNgMHTYe3lcKeTl5RETE0N0dDQ6nY7k5GR27tzp6litCg8Pp1evXgD4+/sTGxtLcXGxi1OdnclkYs+ePYwZM8bVUc6qurqan3/+mdGjRwOg0+nc5lthWxRFob6+nsbGRurr6wkPD3d1pGb69evXYitg586djBgxAoARI0a4zb+11rIOHDgQLy8vABITE93q31preQHefPNNbrrpJjQaTYe9l8ftPiouLkav19tu6/V6cnNzXZjIPgUFBRw+fJiEhARXRzmrNWvWMHPmTGpqalwd5awKCgoICQlh5cqVHD16lF69enHLLbfg5+fn6mitioiIYNKkSdx11134+PgwcOBABg4c6OpY51RWVmYrr/DwcMrLy12cyD7/+c9/SE5OdnWMs9q1axcRERH06NGjQ1/X47YUWhuB25Et6wi1tbUsWbKEW265hYCAAFfHadPu3bsJDQ21bd24s8bGRg4fPszYsWN55pln8PX1dZtdG62prKxk586dvPTSS7z66qvU1taybds2V8fqlD744AO8vLy4+uqrXR2lTXV1dXzwwQdMnz69w1/b40pBr9djMplst00mk9tthp/ObDazZMkSrr76aoYNG+bqOGf1yy+/sGvXLu6++26WLVvGjz/+yIoVK1wdq1V6vR69Xo/RaATg8ssv5/Dhwy5O1bZ9+/YRFRVFSEgIOp2OYcOGkZOT4+pY5xQaGkpJSQkAJSUlHXIg1JG+/PJLdu/ezbx589z6y+LJkycpKChg/vz53H333ZhMJh566CFKS0sv+LU9bvdRfHw8+fn5FBQUEBERQXZ2NvPmzXN1rFZZLBZeeeUVYmNjueaaa1wd55xmzJjBjBkzAPjpp5/4+OOP3fazDQsLQ6/Xc/z4cbp27cq+ffvo1q2bq2O1yWAwkJubS11dHT4+Puzbt4/4+HhXxzqnpKQktm7dSmpqKlu3bmXo0KGujtSmvXv38u9//5vHH38cX19fV8c5q7i4OFatWmW7fffdd/PUU091SOl65BnNe/bs4c0330RRFEaNGsW1117r6kitOnDgAI8++ihxcXG2by033ngjQ4YMcXGyc2sqBXceknrkyBFeeeUVzGYzUVFRzJkzxy2GS7bl3XffJTs7Gy8vL3r06MGdd96Jt7e3q2PZLFu2jP3791NRUUFoaCjTpk1j6NChLF26lKKiIgwGA/fff79bfMatZV2/fj1ms9mWz2g0cvvtt7s4qVVreZsGSYCUghBCCAfxuGMKQggh2ialIIQQwkZKQQghhI2UghBCCBspBSGEEDZSCkK4wLRp0zhx4oSrYwjRgsedvCZEa+6++25KS0vRak99Txo5ciSzZ892YSohnE9KQYjfPfTQQwwYMMDVMYRwKSkFIc7iyy+/ZPPmzfTs2ZOtW7cSHh7O7Nmz6d+/P2Cddff111/nwIEDBAUFMWXKFNsF1RVF4cMPP2TLli2UlZXRpUsX5s+fb5v7/ocffuDvf/87FRUVXHnllcyePRuNRsOJEyd4+eWXOXLkCDqdjksuuYT77rvPZZ+B8CxSCkKcQ25uLsOGDSMjI4Nvv/2W5557jpdeeomgoCCWL19O9+7defXVVzl+/DhPPPEE0dHR9O/fn08++YSsrCwefvhhunTpwtGjR5vNqbNnzx6eeuopampqeOihh0hKSmLQoEG88847DBw4kMWLF2M2mzl06JALf3vhaaQUhPjds88+a7vICsDMmTPR6XSEhoYyceJENBoNycnJfPzxx+zZs4d+/fpx4MABFixYgI+PDz169GDMmDFs27aN/v37s3nzZmbOnEnXrl0BWsx7n5qaSmBgIIGBgVx88cUcOXKEQYMGodPpKCwspKSkBL1eT58+fZz5MQgPJ6UgxO/mz5/f4pjCl19+SURERLNplCMjIykuLqakpISgoCD8/f1tjxkMBg4ePAhYp2WPjo5u8/3CwsJsP/v6+lJbWwtYy+idd95h4cKFBAYGcs011zSb/EwIR5JSEOIciouLsVgstmIoKioiKSmJ8PBwKisrqampsRVDUVERERERgPWaDSdPniQuLu683i8sLIw777wTsM6U+8QTT9CvXz9iYmI68LcSonVynoIQ51BWVsaGDRswm83s2LGD3377jcGDB2MwGOjduzf/+te/qK+v5+jRo2zZssV2xa4xY8awdu1a8vPzsVgsHD16lIqKinO+344dO2wXgmq6bvTpQ2WFcCTZUhDid08//XSzle+AAQMYOnQoRqOR/Px8Zs+eTVhYGPfffz/BwcEA3Hvvvbz++uvccccdBAUFcf3119t2QV1zzTU0NDTw5JNPUlFRQWxsLA8++OA5cxw8eJA1a9ZQXV1NWFgYt956K1FRUY75pYU4g1xPQYizaBqS+sQTT7g6ihBOIdukQgghbKQUhBBC2MjuIyGEEDaypSCEEMJGSkEIIYSNlIIQQggbKQUhhBA2UgpCCCFs/j/ZgJRHACGqDAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "# Define early_stopping_monitor\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "# Add the first and second layers\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Fit model_1\n",
    "model_1_training = model.fit(predictors, target_oh , epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model_2\n",
    "model_2_training = model_2.fit(predictors, target_oh , epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding layers to a network\n",
    "\n",
    "You've seen how to experiment with wider networks. In this exercise, you'll try a deeper network (more hidden layers).\n",
    "\n",
    "Once again, you have a baseline model called model_1 as a starting point. It has 1 hidden layer, with 50 units. You can see a summary of that model's structure printed out. You will create a similar network with 3 hidden layers (still keeping 50 units in each layer).\n",
    "\n",
    "This will again take a moment to fit both models, so you'll need to wait a few seconds to see the results after you run your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The input shape to use in the first hidden layer\n",
    "input_shape = (n_cols,)\n",
    "\n",
    "# Create the new model: model_2\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Add the first, second, and third hidden layers\n",
    "model_2.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "model_2.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model_2\n",
    "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model 1\n",
    "model_1_training = model.fit(predictors, target_oh , epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Fit model 2\n",
    "model_2_training = model_2.fit(predictors,  target_oh , epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification \n",
    "## A binary classification model\n",
    "\n",
    "Now that you know what the Banknote Authentication dataset looks like, we'll build a simple model to distinguish between real and fake bills.\n",
    "\n",
    "You will perform binary classification by using a single neuron as an output. The input layer will have 4 neurons since we have 4 features in our dataset. The model's output will be a value constrained between 0 and 1.\n",
    "\n",
    "We will interpret this output number as the probability of our input variables coming from a fake dollar bill, with 1 meaning we are certain it's a fake bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "'''def convert_to_xlsx():\n",
    "    with open('sample.xls') as xml_file:\n",
    "        soup = BeautifulSoup(xml_file.read(), 'xml')\n",
    "        writer = pd.ExcelWriter('sample.xlsx')\n",
    "        for sheet in soup.findAll('Worksheet'):\n",
    "            sheet_as_list = []\n",
    "            for row in sheet.findAll('Row'):\n",
    "                sheet_as_list.append([cell.Data.text if cell.Data else '' for cell in row.findAll('Cell')])\n",
    "            pd.DataFrame(sheet_as_list).to_excel(writer, sheet_name=sheet.attrs['ss:Name'], index=False, header=False)\n",
    "\n",
    "        writer.save()'''\n",
    "\n",
    "banknotes = pd.read_excel('/home/abderrazak/ALLINHERE/NLP/Datacamp/banknotes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Use pairplot and set the hue to be our class column\n",
    "sns.pairplot(banknotes, hue='class') \n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Describe the data\n",
    "print('Dataset stats: \\n', banknotes.describe())\n",
    "\n",
    "# Count the number of observations per class\n",
    "print('Observations per class: \\n', banknotes['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Lesson2\n",
    "Hello nets!\n",
    "\n",
    "You're going to build a simple neural network to get a feeling of how quickly it is to accomplish this in Keras.\n",
    "\n",
    "You will build a network that takes two numbers as an input, passes them through a hidden layer of 10 neurons, and finally outputs a single non-constrained number.\n",
    "\n",
    "\n",
    "A non-constrained output can be obtained by avoiding setting an activation function in the output layer. This is useful for problems like regression, when we want our output to be able to take any non-constrained value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with five neurons and three inputs\n",
    "model.add(Dense(5, input_shape=(3,), activation=\"relu\"))\n",
    "\n",
    "# Add a final Dense layer with one neuron and no activation\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Summarize your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a Dense layer with 50 neurons and an input of 1 neuron\n",
    "model.add(Dense(50, input_shape=(1,), activation='relu'))\n",
    "\n",
    "# Add two Dense layers with 50 neurons and relu activation\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "\n",
    "# End your model with a Dense layer and no activation\n",
    "model.add(Dense(1))\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss= 'mse')\n",
    "\n",
    "print(\"Training started..., this can take a while:\")\n",
    "\n",
    "# Fit your model on your data for 30 epochs\n",
    "model.fit(time_steps,y_positions, epochs = 30)\n",
    "\n",
    "# Evaluate your model \n",
    "print(\"Final loss value:\",model.evaluate(time_steps, y_positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the twenty minutes orbit\n",
    "twenty_min_orbit = model.predict(np.arange(-10, 11))\n",
    "\n",
    "# Plot the twenty minute orbit \n",
    "plot_orbit(twenty_min_orbit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification\n",
    "## A binary classification model\n",
    "\n",
    "Now that you know what the Banknote Authentication dataset looks like, we'll build a simple model to distinguish between real and fake bills.\n",
    "\n",
    "You will perform binary classification by using a single neuron as an output. The input layer will have 4 neurons since we have 4 features in our dataset. The model's output will be a value constrained between 0 and 1.\n",
    "\n",
    "We will interpret this output number as the probability of our input variables coming from a fake dollar bill, with 1 meaning we are certain it's a fake bill.\n",
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "bankotes\n",
    "# Use pairplot and set the hue to be our class column\n",
    "sns.pairplot(banknotes, hue='class') \n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Describe the data\n",
    "print('Dataset stats: \\n', banknotes.describe())\n",
    "\n",
    "# Count the number of observations per class\n",
    "print('Observations per class: \\n', banknotes['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sequential model and dense layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "\n",
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a dense layer \n",
    "model.add(Dense(1, input_shape=(4,), activation='sigmoid'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Display a summary of your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is this dollar bill fake ?\n",
    "\n",
    "You are now ready to train your model and check how well it performs when classifying new bills! The dataset has already been partitioned into features: X_train & X_test, and labels: y_train & y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 20 epochs\n",
    "model.fit(X_train, y_train, epochs = 20)\n",
    "\n",
    "# Evaluate your model accuracy on the test set\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multi-class model\n",
    "\n",
    "You're going to build a model that predicts who threw which dart only based on where that dart landed! (That is the dart's x and y coordinates on the board.)\n",
    "\n",
    "This problem is a multi-class classification problem since each dart can only be thrown by one of 4 competitors. So classes/labels are mutually exclusive, and therefore we can build a neuron with as many output as competitors and use the softmax activation function to achieve a total sum of probabilities of 1 over all competitors.\n",
    "\n",
    "Keras Sequential model and Dense layer are already loaded for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare your dataset\n",
    "\n",
    "In the console you can check that your labels, darts.competitor are not yet in a format to be understood by your network. They contain the names of the competitors as strings. You will first turn these competitors into unique numbers,then use the to_categorical() function from keras.utils to turn these numbers into their one-hot encoded representation.\n",
    "\n",
    "This is useful for multi-class classification problems, since there are as many output neurons as classes and for every observation in our dataset we just want one of the neurons to be activated.\n",
    "\n",
    "The dart's dataset is loaded as darts. Pandas is imported as pd. Let's prepare this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare your dataset\n",
    "\n",
    "In the console you can check that your labels, darts.competitor are not yet in a format to be understood by your network. They contain the names of the competitors as strings. You will first turn these competitors into unique numbers,then use the to_categorical() function from keras.utils to turn these numbers into their one-hot encoded representation.\n",
    "\n",
    "This is useful for multi-class classification problems, since there are as many output neurons as classes and for every observation in our dataset we just want one of the neurons to be activated.\n",
    "\n",
    "The dart's dataset is loaded as darts. Pandas is imported as pd. Let's prepare this dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import to_categorical from keras utils module\n",
    "from keras.utils import  to_categorical\n",
    "\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "# Use to_categorical on your labels\n",
    "competitors = to_categorical(darts.competitor)\n",
    "\n",
    "# Now print the one-hot encoded labels\n",
    "print('One-hot encoded competitors: \\n',competitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on dart throwers\n",
    "\n",
    "Your model is now ready, just as your dataset. It's time to train!\n",
    "\n",
    "The coordinates features and competitors labels you just transformed have been partitioned into coord_train,coord_test and competitors_train,competitors_test.\n",
    "\n",
    "Your model is also loaded. Feel free to visualize your training data or model.summary() in the console.\n",
    "\n",
    "Let's find out who threw which dart just by looking at the board!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your model to the training data for 200 epochs\n",
    "model.fit(coord_train,competitors_train,epochs=200)\n",
    "\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax predictions\n",
    "\n",
    "Your recently trained model is loaded for you. This model is generalizing well!, that's why you got a high accuracy on the test set.\n",
    "\n",
    "Since you used the softmax activation function, for every input of 2 coordinates provided to your model there's an output vector of 4 numbers. Each of these numbers encodes the probability of a given dart being thrown by one of the 4 possible competitors.\n",
    "\n",
    "When computing accuracy with the model's .evaluate() method, your model takes the class with the highest probability as the prediction. np.argmax() can help you do this since it returns the index with the highest value in an array.\n",
    "\n",
    "Use the collection of test throws stored in coords_small_test and np.argmax()to check this out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the position of highest probability from each pred vector\n",
    "preds_chosen = [np.argmax(pred)for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds_chosen):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification\n",
    "\n",
    "\n",
    "Now that you know how multi-class classification works, we can take a look at multi-label classification. They both deal with predicting classes, but in multi-label classification, a single input can be assigned to more than one class.\n",
    "2. Real world examples\n",
    "\n",
    "We could use multi-label classification, for instance, to tag a series genres by its plot summary.\n",
    "3. Real world examples\n",
    "\n",
    "Making models that deal with text or images is not covered in this course in depth. We will learn more about it on chapter 4.\n",
    "4. Multi-class vs multi-label\n",
    "\n",
    "Imagine we had three classes; sun, moon and clouds. In multi-class problems if we took a sample of our observations each individual in the sample will belong to a unique class. However in a multi-label problem each individual in the sample can have all, none or a subset of the available classes. As you can see in the image, multi-label vectors are also one-hot encoded, there's a 1 or a 0 representing the presence or absence of each class.\n",
    "\n",
    "    1 https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "\n",
    "5. The architecture\n",
    "\n",
    "Making a multi-label model for this problem is not very different to what you did when building your multi-class model. We first instantiate a sequential model. For the sake of this example, we will assume that to differentiate between these 3 classes, we need just one input and 2 hidden neurons. The biggest changes happen in the output layer and in its activation function. In the output layer, we use as many neurons as possible classes but we use sigmoid activation this time.\n",
    "6. Sigmoid outputs\n",
    "\n",
    "We use sigmoid outputs because we no longer care about the sum of probabilities. We want each output neuron to be able to individually take a value between 0 and 1. This can be achieved with the sigmoid activation because it constrains our neuron output in the range 0-1. That's what we did in binary classification, though we only had one output neuron there.\n",
    "7. Compile and train\n",
    "\n",
    "Binary cross-entropy is now used as the loss function when compiling the model. You can look at it as if you were performing several binary classification problems: for each output we are deciding whether or not its corresponding label is present. When training our model we can use the validation_split argument to print validation loss and accuracy as it trains. By using validation_split, a percentage of training data is left out for testing at each epoch.\n",
    "8. An advantage\n",
    "\n",
    "Using neural networks for multi-label classification can be performed by minor tweaks in our model architecture. If we were to use a classical machine learning approach to solve multi-label problems we would need more complex methods. One way to do so consists of training several classifiers to distinguish each particular class from the rest. This is called one versus rest classification.\n",
    "9. An irrigation machine\n",
    "\n",
    "Let's tackle a new problem. A farm field has an array of 20 sensors distributed along 3 crop fields. These sensors measure, among other things, the humidity of the soil, radiation of the sun, etc. Your task is to use the combination of measurements of these sensors to decide which parcels to water, given each parcel has different environmental requirements.\n",
    "10. An irrigation machine\n",
    "\n",
    "Each sensor measures an integer value between 0 and 13 volts. Parcels can be represented as one-hot encoded vectors of length 3, where each index is one of the parcels. Parcels can be watered simultaneously.\n",
    "11. Let's practice!\n",
    "\n",
    "Let's put what you just learned to the test! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64, input_shape=(20,), activation='relu'))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "\n",
    "# Compile your model with binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "           loss = 'binary_crossentropy',\n",
    "           metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with multiple labels\n",
    "\n",
    "An output of your multi-label model could look like this: [0.76 , 0.99 , 0.66 ]. If we round up probabilities higher than 0.5, this observation will be classified as containing all 3 possible labels [1,1,1]. For this particular problem, this would mean watering all 3 parcels in your farm is the right thing to do, according to the network, given the input sensor measurements.\n",
    "\n",
    "You will now train and predict with the model you just built. sensors_train, parcels_train, sensors_test and parcels_test are already loaded for you to use.\n",
    "\n",
    "Let's see how well your intelligent machine performs!\n",
    "Hint\n",
    "\n",
    "    The .fit() method needs the features and the labels of the training data as parameters.\n",
    "    The .predict() method receives the features of the test data as parameters.\n",
    "    np.round() takes an array of predictions and rounds them up.\n",
    "    The .evaluate() method receives the features and the labels of the test data as parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, epochs = 100, validation_split = 0.2)\n",
    "\n",
    "# Predict on sensors_test and round up the predictions\n",
    "preds = model.predict(sensors_test)\n",
    "preds_rounded = np.round(preds)\n",
    "\n",
    "# Print rounded preds\n",
    "print('Rounded Predictions: \\n', preds_rounded)\n",
    "\n",
    "# Evaluate your model's accuracy on the test data\n",
    "accuracy = model.evaluate( parcels_test,parcels_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras callbacks\n",
    "1. Keras callbacks\n",
    "\n",
    "By now you've trained a lot of models. It's time to learn more about how to better control and supervise model training by using callbacks.\n",
    "2. What is a callback?\n",
    "\n",
    "A callback is a function that is executed after some other function, event, or task has finished. For instance, when you touch your phone screen, a block of code that identifies the type of gesture will be triggered. Since this block of code has been called after the touching event occurred, it's a callback.\n",
    "3. Callbacks in Keras\n",
    "\n",
    "In the same way, a keras callback is a block of code that gets executed after each epoch during training or after the training is finished. They are useful to store metrics as the model trains and to make decisions as the training goes by.\n",
    "4. A callback you've been missing\n",
    "\n",
    "Every time you call the fit method on a keras model there's a callback object that gets returned after the model finishes training. This is the history object. Accessing the history attribute, which is a python dictionary,we can check the saved metrics of the model at each epoch during training as an array of numbers.\n",
    "5. A callback you've been missing\n",
    "\n",
    "To get the most out of the history object we should use the validation_data parameter in our fit method, passing X_test and y_test as a tuple. The validation_split parameter can be used instead, specifying a percentage of the training data that will be left out for testing purposes. That way we not only have the training metrics but also the validation metrics.\n",
    "6. History plots\n",
    "\n",
    "You can compare training and validation metrics with a few matplotlib commands. We just need to define a figure. Plot the values of the history attribute for the training accuracy (acc) and the validation accuracy (val_acc). We can then make our graph prettier by adding a title, axis labels and a legend.\n",
    "7. History plots\n",
    "\n",
    "We can see our model accuracy increases for both training and test sets till it reaches epoch 25. Then accuracy flattens for the test set whilst the training keeps improving. Overfitting it's taking place since we see the train keeps improving as test data decreases in accuracy. More on this in the next chapter.\n",
    "8. Early stopping\n",
    "\n",
    "Early stopping a model can solve the overfitting problem. Since it stops its training when it no longer improves. This is extremely useful since deep neural models can take a long time to train and we don't know beforehand how many epochs will be needed. Early stopping, like other keras callbacks can be imported from keras callbacks. We then need to instantiate it. The early stopping callback can monitor several metrics, like validation accuracy, validation loss, etc. These can be specified in the monitor parameter. It's also important to define a patience argument, that is the number of epochs to wait for the model to improve before stopping it's training. There's no rules to decide which patience number works best at all times,this depends on the implementation. It's good to avoid low values, that way your model has a chance to improve at a later epoch.The callback is passed as a list to the callbacks parameter in the model fit method.\n",
    "9. Model checkpoint\n",
    "\n",
    "The model checkpoint callback can also be imported from keras callbacks. This callback allows us to save our model as it trains. We specify the model filename with a name and the .hdf5 extension. You can also decide what to monitor to determine which model is best with the monitor parameter, by default validation loss is monitored. Setting the save_best_only parameter to True guarantees that the latest best model according to the quantity monitored will not be overwritten. \n",
    "\n",
    "# The history callback\n",
    "\n",
    "The history callback is returned by default every time you train a model with the .fit() method. To access these metrics you can access the history dictionary parameter inside the returned h_callback object with the corresponding keys.\n",
    "\n",
    "The irrigation machine model you built in the previous lesson is loaded for you to train, along with its features and labels now loaded as X_train, y_train, X_test, y_test. This time you will store the model's historycallback and use the validation_data parameter as it trains.\n",
    "\n",
    "You will plot the results stored in history with plot_accuracy() and plot_loss(), two simple matplotlib functions. You can check their code in the console by pasting show_code(plot_loss).\n",
    "\n",
    "Let's see the behind the scenes of our training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and save its history\n",
    "h_callback = model.fit(X_train, y_train, epochs = 50,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['acc'], h_callback.history['val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early stopping your model\n",
    "\n",
    "The early stopping callback is useful since it allows for you to stop the model training if it no longer improves after a given number of epochs. To make use of this functionality you need to pass the callback inside a list to the model's callback parameter in the .fit() method.\n",
    "\n",
    "The model you built to detect fake dollar bills is loaded for you to train, this time with early stopping. X_train, y_train, X_test and y_test are also available for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the early stopping callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Define a callback to monitor val_acc\n",
    "monitor_val_acc = EarlyStopping(monitor='val_acc', \n",
    "                       patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train,y_train, \n",
    "           epochs=1000, validation_data=(X_test,y_test),\n",
    "           callbacks= [monitor_val_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A combination of callbacks\n",
    "\n",
    "Deep learning models can take a long time to train, especially when you move to deeper architectures and bigger datasets. Saving your model every time it improves as well as stopping it when it no longer does allows you to worry less about choosing the number of epochs to train for. You can also restore a saved model anytime and resume training where you left it.\n",
    "\n",
    "The model training and validation data are available in your workspace as X_train, X_test, y_train, and y_test.\n",
    "\n",
    "Use the EarlyStopping() and the ModelCheckpoint() callbacks so that you can go eat a jar of cookies while you leave your computer to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor = 'val_acc', patience = 3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "modelCheckpoint =  ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "h_callback = model.fit(X_train, y_train,\n",
    "                    epochs = 1000000000000,\n",
    "                    callbacks = [monitor_val_acc, modelCheckpoint],\n",
    "                    validation_data = (X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning curves\n",
    "\n",
    "You're going to build a model on the digits dataset, a sample dataset that comes pre-loaded with scikit learn. The digits dataset consist of 8x8 pixel handwritten digits from 0 to 9:\n",
    "You want to distinguish between each of the 10 possible digits given an image, so we are dealing with multi-class classification.\n",
    "\n",
    "The dataset has already been partitioned into X_train, y_train, X_test, and y_test, using 30% of the data as testing data. The labels are already one-hot encoded vectors, so you don't need to use Keras to_categorical() function.\n",
    "\n",
    "Let's build this new model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Input and hidden layer with input_shape, 16 neurons, and relu \n",
    "model.add(Dense(16, input_shape = (64,), activation = 'relu'))\n",
    "\n",
    "# Output layer with 10 neurons (one per digit) and softmax\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Test if your model is well assembled by predicting before training\n",
    "print(model.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is the model overfitting?\n",
    "\n",
    "Let's train the model you just built and plot its learning curve to check out if it's overfitting! You can make use of the loaded function plot_loss() to plot training loss against validation loss, you can get both from the history callback.\n",
    "\n",
    "If you want to inspect the plot_loss() function code, paste this in the console: show_code(plot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "h_callback = model.fit(X_train, y_train, epochs = 60, validation_data = (X_test,y_test), verbose=0)\n",
    "\n",
    "# Extract from the h_callback object loss and val_loss to plot the learning curve\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph doesn't show overfitting but convergence. It looks like your model has learned all it could from the data and it no longer improves. The test loss, although higher than the training loss, is not getting worse, so we aren't overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do we need more data?\n",
    "\n",
    "It's time to check whether the digits dataset model you built benefits from more training examples!\n",
    "\n",
    "In order to keep code to a minimum, various things are already initialized and ready to use:\n",
    "\n",
    "    The model you just built.\n",
    "    X_train,y_train,X_test, and y_test.\n",
    "    The initial_weights of your model, saved after using model.get_weights().\n",
    "    A pre-defined list of training sizes: training_sizes.\n",
    "    A pre-defined early stopping callback monitoring loss: early_stop.\n",
    "    Two empty lists to store the evaluation results: train_accs and test_accs.\n",
    "\n",
    "Train your model on the different training sizes and evaluate the results on X_test. End by plotting the results with plot_results().\n",
    "\n",
    "The full code for this exercise can be found on the slid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in training_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "\n",
    "    # Reset the model to the initial weights and train it on the new training data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store both: the training data fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_train,y_train)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "## Comparing activation functions\n",
    "\n",
    "Comparing activation functions involves a bit of coding, but nothing you can't do!\n",
    "\n",
    "You will try out different activation functions on the multi-label model you built for your farm irrigation machine in chapter 2. The function get_model('relu') returns a copy of this model and applies the 'relu' activation function to its hidden layer.\n",
    "\n",
    "You will loop through several activation functions, generate a new model for each and train it. By storing the history callback in a dictionary you will be able to visualize which activation function performed best in the next exercise!\n",
    "\n",
    "X_train, y_train, X_test, y_test are ready for you to use when training your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "    # Get a new model with the current activation\n",
    "    model = get_model(act)\n",
    "    # Fit the model and store the history results\n",
    "    h_callback =model.fit(X_train,y_train, validation_data=(X_test,y_test),epochs= 20, verbose=0 )\n",
    "    activation_results[act] = h_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing activation functions II\n",
    "\n",
    "What you coded in the previous exercise has been executed to obtain theactivation_results variable, this time 100 epochs were used instead of 20. This way you will have more epochs to further compare how the training evolves per activation function.\n",
    "\n",
    "For every h_callback of each activation function in activation_results:\n",
    "\n",
    "    The h_callback.history['val_loss'] has been extracted.\n",
    "    The h_callback.history['val_acc'] has been extracted.\n",
    "\n",
    "Both are saved into two dictionaries: val_loss_per_function and val_acc_per_function.\n",
    "\n",
    "Pandas is also loaded as pd for you to use. Let's plot some quick validation loss and accuracy charts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from val_loss_per_function\n",
    "val_loss= pd.DataFrame(val_loss_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_loss.plot()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe from val_acc_per_function\n",
    "val_acc = pd.DataFrame(val_acc_per_function)\n",
    "\n",
    "# Call plot on the dataframe\n",
    "val_acc.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch size and batch normalization\n",
    "1. Batch size and batch normalization\n",
    "\n",
    "Its time to learn the concepts of batch size and batch normalization.\n",
    "2. Batches\n",
    "\n",
    "A mini-batch is a subset of data samples. If we were training a neural network with images, each image in our training set would be a sample and we could take mini-batches of different sizes from the training set batch.\n",
    "3. Mini-batch\n",
    "\n",
    "Remember that during an epoch we feed our network, calculate the errors and update the network weights. It's not very practical to update our network weights only once per epoch after looking at the error produced by all training samples. In practice, we take a mini-batch of training samples. That way, if our training set has 9 images and we choose a batch_size of 3, we will perform 3 weight updates per epoch, one per mini-batch.\n",
    "4. Mini-batches\n",
    "\n",
    "Networks tend to train faster with mini-batches since weights are updated often. Sometimes datasets are so huge that they would struggle to fit in RAM memory if we didn't use mini-batches. Also, the noise produced by a small batch-size can help escape local minima. A couple of disadvantages are the need for more iterations and finding a good batch size.\n",
    "5. Effects of batch sizes\n",
    "\n",
    "Here you can see how different batch sizes converge towards a minimum as training goes by. Training with all samples is shown in blue. Mini-batching is shown in green. Stochastic gradient descent, in red, uses a batch_size of 1. We can see how the path towards the best value for our weights is noisier the smaller the batch_size. They reach the same value after a different number of iterations.\n",
    "\n",
    "    1 Stack Exchange\n",
    "\n",
    "6. Batch size in Keras\n",
    "\n",
    "You can set your own batch_size with the batch_size parameter on the model's fit method. Keras uses a default batch-size of 32. Increasing powers of two tend to be used. As a rule of thumb, you tend to make your batch size bigger the bigger your dataset.\n",
    "7. Normalization in machine learning\n",
    "\n",
    "Normalization is a common pre-processing step in machine learning algorithms, especially when features have different scales. One way to normalize data is to subtract its mean value and divide by the standard deviation.We always tend to normalize our model inputs. This avoids problems with activation functions and gradients.\n",
    "8. Normalization in machine learning\n",
    "\n",
    "This leaves everything centered around 0 with a standard deviation of 1.\n",
    "9. Reasons for batch normalization\n",
    "\n",
    "Normalizing neural networks inputs improve our model. But deeper layers are trained based on previous layer outputs and since weights get updated via gradient descent, consecutive layers no longer benefit from normalization and they need to adapt to previous layers' weight changes, finding more trouble to learn their own weights. Batch normalization makes sure that, independently of the changes, the inputs to the next layers are normalized. It does this in a smart way, with trainable parameters that also learn how much of this normalization is kept scaling or shifting it.\n",
    "10. Batch normalization advantages\n",
    "\n",
    "This improves gradient flow, allows for higher learning rates, reduces weight initializations dependence, adds regularization to our network and limits internal covariate shift; which is a funny name for a layer's dependence on the previous layer outputs when learning its weights. Batch normalization is widely used today in many deep learning models.\n",
    "11. Batch normalization in Keras\n",
    "\n",
    "Batch normalization in Keras is applied as a layer. So we can place it in between two layers. We import batch normalization from Keras layers. We then instantiate a sequential model, add an input layer, and then add a batch normalization layer. We finalize this binary classification model with an output layer. \n",
    "\n",
    "# Changing batch sizes\n",
    "\n",
    "You've seen models are usually trained in batches of a fixed size. The smaller a batch size, the more weight updates per epoch, but at a cost of a more unstable gradient descent. Specially if the batch size is too small and it's not representative of the entire training set.\n",
    "\n",
    "Let's see how different batch sizes affect the accuracy of a simple binary classification model that separates red from blue dots.\n",
    "\n",
    "You'll use a batch size of one, updating the weights once per sample in your training set for each epoch. Then you will use the entire dataset, updating the weights only once per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh new model with get_model\n",
    "model =get_model()\n",
    "\n",
    "# Train your model for 5 epochs with a batch size of 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
    "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
    "      model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "# Fit your model for 5 epochs with a batch of size the training set\n",
    "model.fit(X_train, y_train, epochs=5,batch_size=len(X_train))\n",
    "print(\"\\n The accuracy when using the whole training set as batch-size was: \",\n",
    "      model.evaluate(X_test, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that accuracy is lower when using a batch size equal to the training set size. This is not because the network had more trouble learning the optimization function: Even though the same number of epochs were used for both batch sizes the number of resulting weight updates was very different!. With a batch of size the training set and 5 epochs we only get 5 updates total, each update computes and averaged gradient descent with all the training set observations. To obtain similar results with this batch size we should increase the number of epochs so that more weight updates take place.\n",
    "# Batch normalizing a familiar model\n",
    "\n",
    "Remember the digits dataset you trained in the first exercise of this chapter?\n",
    "A multi-class classification problem that you solved using softmax and 10 neurons in your output layer.\n",
    "\n",
    "You will now build a new deeper model consisting of 3 hidden layers of 50 neurons each, using batch normalization in between layers. The kernel_initializer parameter is used to initialize weights in a similar w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import batch normalization from keras layers\n",
    "from keras.layers import  BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch normalization effects\n",
    "\n",
    "Batch normalization tends to increase the learning speed of our models and make their learning curves more stable. Let's see how two identical models with and without batch normalization compare.\n",
    "\n",
    "The model you just built batchnorm_model is loaded for you to use. An exact copy of it without batch normalization: standard_model, is available as well. You can check their summary() in the console. X_train, y_train, X_test, and y_test are also loaded so that you can train both models.\n",
    "\n",
    "You will compare the accuracy learning curves for both models plotting them with compare_histories_acc().\n",
    "\n",
    "You can check the function pasting show_code(compare_histories_acc) in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_histories_acc(h1,h2):\n",
    "    plt.plot(h1.history['acc'])\n",
    "    plt.plot(h1.history['val_acc'])\n",
    "    plt.plot(h2.history['acc'])\n",
    "    plt.plot(h2.history['val_acc'])\n",
    "    plt.title(\"Batch Normalization Effects\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Test', 'Train with Batch Normalization', 'Test with Batch Normalization'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your standard model, storing its history callback\n",
    "h1_callback = standard_model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Train the batch normalized model you recently built, store its history callback\n",
    "h2_callback = batchnorm_model.fit(X_train,y_train, validation_data=(X_test,y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Call compare_histories_acc passing in both model histories\n",
    "compare_histories_acc(h1_callback, h2_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a model for tuning\n",
    "\n",
    "Let's tune the hyperparameters of a binary classification model that does well classifying the breast cancer dataset.\n",
    "\n",
    "You've seen that the first step to turn a model into a sklearn estimator is to build a function that creates it. The definition of this function is important since hyperparameter tuning is carried out by varying the arguments your function receives.\n",
    "\n",
    "Build a simple create_model() function that receives both a learning rate and an activation function as arguments. The Adam optimizer has been imported as an object from keras.optimizers so that you can also change its learning rate parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a model given an activation and learning rate\n",
    "def create_model(learning_rate, activation):\n",
    "  \n",
    "  \t# Create an Adam optimizer with the given learning rate\n",
    "  \topt = Adam(lr =learning_rate)\n",
    "  \t\n",
    "  \t# Create your binary classification model  \n",
    "  \tmodel = Sequential()\n",
    "  \tmodel.add(Dense(128, input_shape = (30,), activation = activation))\n",
    "  \tmodel.add(Dense(256, activation = activation))\n",
    "  \tmodel.add(Dense(1, activation = 'sigmoid'))\n",
    "  \t\n",
    "  \t# Compile your model with your optimizer, loss, and metrics\n",
    "  \tmodel.compile(optimizer = opt, loss = 'binarycross_entropy', metrics = ['accuracy'])\n",
    "  \treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the model parameters\n",
    "\n",
    "It's time to try out different parameters on your model and see how well it performs!\n",
    "\n",
    "The create_model() function you built in the previous exercise is ready for you to use.\n",
    "\n",
    "Since fitting the RandomizedSearchCV object would take too long, the results you'd get are printed in the show_results() function. You could try random_search.fit(X,y) in the console yourself to check it does work after you have built everything else, but you will probably timeout the exercise (so copy your code first if you try this or you can lose your progress!).\n",
    "\n",
    "You don't need to use the optional epochs and batch_size parameters when building your KerasClassifier object since you are passing them as params to the random search and this works already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras scikit learn wrappers\n",
    "from keras.wrappers.scikit_learn import  KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model)\n",
    "\n",
    "# Define the parameters to try out\n",
    "params = {'activation': ['relu', 'tanh'], 'batch_size': [ 32, 128, 256], \n",
    "          'epochs': [50, 100, 200 ], 'learning_rate': [ 0.1, 0.01,  0.001]}\n",
    "\n",
    "# Create a randomize search cv object passing in the parameters to try\n",
    "random_search = RandomizedSearchCV(model, param_distributions = params, cv = KFold(3))\n",
    "\n",
    "# Running random_search.fit(X,y) would start the search,but it takes too long! \n",
    "show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with cross-validation\n",
    "\n",
    "Time to train your model with the best parameters found: 0.001 for the learning rate, 50 epochs, a 128 batch_size and relu activations.\n",
    "\n",
    "The create_model() function from the previous exercise is ready for you to use. X and y are loaded as features and labels.\n",
    "\n",
    "Use the best values found for your model when creating your KerasClassifier object so that they are used when performing cross_validation.\n",
    "\n",
    "End this chapter by training an awesome tuned model on the breast cancer dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KerasClassifier from keras wrappers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Create a KerasClassifier\n",
    "model = KerasClassifier(build_fn = create_model(learning_rate =0.001, activation = 'relu'), epochs =50, \n",
    "             batch_size = 128, verbose = 0)\n",
    "\n",
    "# Calculate the accuracy score for each fold\n",
    "kfolds = cross_val_score(model,X, y, cv = 3)\n",
    "\n",
    "# Print the mean accuracy\n",
    "print('The mean accuracy was:', kfolds.mean())\n",
    "\n",
    "# Print the accuracy standard deviation\n",
    "print('With a standard deviation of:', kfolds.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tensors, layers and autoencoders\n",
    "\n",
    "Now that you know how to tune your models,it's time to better understand how they work and learn about new neural network architectures.\n",
    "2. Accessing Keras layers\n",
    "\n",
    "Model layers are easily accessible, we just need to call layers on a built model and access the index of the layer we want. From a chosen layer we can print its inputs, outputs, and weights. You can see inputs and outputs are tensors of a given shape built with TensorFlow tensor objects, weights are TensorFlow variable objects, which are just tensors that change their value as the neural network learns the best weights.\n",
    "3. What are tensors?\n",
    "\n",
    "Tensors are the main data structures used in deep learning, inputs, outputs, and transformations in neural networks are all represented using tensors. A tensor is a multi-dimensional array of numbers. A 2 dimensional tensor is a matrix, a 3 dimensional tensor is an array of matrices.\n",
    "4. Keras backend\n",
    "\n",
    "If we import the Keras backend we can build a function that takes in an input tensor from a given layer and returns an output tensor from another or the same layer. Tensorflow is the backend Keras is using in this course, but it could be any other, like Theano. To define the function with our backend K we need to give it a list of inputs and outputs, even if we just want 1 input and 1 output. Then we can use it on a tensor with the same shape as the input layer given during its definition. If the weights of the layers between our input and outputs change the function output for the same input will change as well. We can use this to see the output of certain layers as weights change during training, we will check this in the exercises!\n",
    "5. Introducing autoencoders\n",
    "\n",
    "It's time to introduce a new architecture.\n",
    "6. Autoencoders!\n",
    "\n",
    "Autoencoders! Autoencoders are models that aim at producing the same inputs as outputs.\n",
    "7. Autoencoders!\n",
    "\n",
    "This task alone wouldn't be very useful, but since along the way we decrease the number of neurons, we are effectively making our network learn to compress its inputs into a small set of neurons.\n",
    "8. Autoencoder use cases\n",
    "\n",
    "This makes autoencoders useful for things like: Dimensionality reduction, since we can obtain a smaller dimensional space representation of our inputs. De-noising, if trained with clear data and then fed with noisy data they will be able to decode back a good representation of the input data without noise. Anomaly detection, if you train an autoencoder to map inputs to outputs with data but you then pass in strange values, the network will fail at giving accurate output values. Many other applications can also benefit from this architecture.\n",
    "9. Building a simple autoencoder\n",
    "\n",
    "To make an autoencoder that maps a hundred inputs to a hundred outputs, encoding the inputs into a layer of 4 neurons, we would do the following: Instantiate a sequential model, add a dense layer of 4 neurons with an input_shape of a hundred and end with an output layer of 100 neurons. We use activation sigmoid because we assume that our output can take a value between 0 and 1, we end compiling our model with adam optimizer and binary_crossentropy loss since we used sigmoid.\n",
    "10. Breaking it into an encoder\n",
    "\n",
    "Once you've built and trained your autoencoder you might want to encode your inputs. To do this, you just have to build a new model and add the first layer of your previously trained autoencoder. This new model predictions returns the 4 numbers given by the 4 neurons of the hidden layer for each observation in the input dataset. \n",
    "# It's a flow of tensors\n",
    "\n",
    "If you have already built a model, you can use the model.layers and the keras.backend to build functions that, provided with a valid input tensor, return the corresponding output tensor.\n",
    "\n",
    "This is a useful tool when we want to obtain the output of a network at an intermediate layer.\n",
    "\n",
    "For instance, if you get the input and output from the first layer of a network, you can build an inp_to_out function that returns the result of carrying out forward propagation through only the first layer for a given input tensor.\n",
    "\n",
    "So that's what you're going to do right now!\n",
    "\n",
    "X_test from the Banknote Authentication dataset and its model are preloaded. Type model.summary() in the console to check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keras backend\n",
    "import keras.backend as K\n",
    "\n",
    "# Input tensor from the 1st layer of the model\n",
    "inp = model.layers[0].input\n",
    "\n",
    "# Output tensor from the 1st layer of the model\n",
    "out = model.layers[0].output\n",
    "\n",
    "# Define a function from inputs to outputs\n",
    "inp_to_out = K.function([inp], [out])\n",
    "\n",
    "# Print the results of passing X_test through the 1st layer\n",
    "print(inp_to_out([X_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural separation\n",
    "\n",
    "Put on your gloves because you're going to perform brain surgery!\n",
    "\n",
    "Neurons learn by updating their weights to output values that help them better distinguish between the different output classes in your dataset. You will make use of the inp_to_out() function you just built to visualize the output of two neurons in the first layer of the Banknote Authentication model as it learns.\n",
    "\n",
    "The model you built in chapter 2 is ready for you to use, just like X_test and y_test. Paste show_code(plot) in the console if you want to check plot().\n",
    "\n",
    "You're performing heavy duty, once all is done, click through the graphs to watch the separation live!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 21):\n",
    "  \t# Train model for 1 epoch\n",
    "    h = model.fit(X_train, y_train, batch_size = 16, epochs = 1, verbose = 0)\n",
    "    if i%4==0: \n",
    "        # Get the output of the first layer\n",
    "        layer_output = inp_to_out([X_test])[0]\n",
    "      \n",
    "        # Evaluate model accuracy for this epoch\n",
    "        test_accuracy = model.evaluate(X_test, y_test)[1] \n",
    "      \n",
    "      # Plot 1st vs 2nd neuron output\n",
    "      plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an autoencoder\n",
    "\n",
    "Autoencoders have several interesting applications like anomaly detection or image denoising. They aim at producing an output identical to its inputs. The input will be compressed into a lower dimensional space, encoded. The model then learns to decode it back to its original form.\n",
    "\n",
    "You will encode and decode the MNIST dataset of handwritten digits, the hidden layer will encode a 32-dimensional representation of the image, which originally consists of 784 pixels (28 x 28). The autoencoder will essentially learn to turn the 784 pixels original image into a compressed 32 pixels image and learn how to use that encoded representation to bring back the original 784 pixels image.\n",
    "\n",
    "The Sequential model and Dense layers are ready for you to use.\n",
    "\n",
    "Let's build an autoencoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a sequential model\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Add a dense layer with input the original image pixels and neurons the encoded representation\n",
    "autoencoder.add(Dense(32, input_shape=(784, ), activation=\"relu\"))\n",
    "\n",
    "# Add an output layer with as many neurons as the orginal image pixels\n",
    "autoencoder.add(Dense(784, activation = \"sigmoid\"))\n",
    "\n",
    "# Compile your model with adadelta\n",
    "autoencoder.compile(optimizer = 'adadelta', loss ='binary_crossentropy')\n",
    "\n",
    "# Summarize your model structure\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De-noising like an autoencoder\n",
    "\n",
    "Okay, you have just built an autoencoder model. Let's see how it handles a more challenging task.\n",
    "\n",
    "First, you will build a model that encodes images, and you will check how different digits are represented with show_encodings(). To build the encoder you will make use of your autoencoder, that has already being trained. You will just use the first half of the network, which contains the input and the bottleneck output. That way, you will obtain a 32 number output which represents the encoded version of the input image.\n",
    "\n",
    "Then, you will apply your autoencoder to noisy images from MNIST, it should be able to clean the noisy artifacts.\n",
    "\n",
    "X_test_noise is loaded in your workspace. The digits in this noisy dataset look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "encodings = encoder.predict( X_test_noise)\n",
    "show_encodings(encodings, number = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your encoder by using the first layer of your autoencoder\n",
    "encoder = Sequential()\n",
    "encoder.add(autoencoder.layers[0])\n",
    "\n",
    "# Encode the noisy images and show the encodings for your favorite number [0-9]\n",
    "encodings = encoder.predict(X_test_noise)\n",
    "show_encodings(encodings, number = 1)\n",
    "\n",
    "# Predict on the noisy images with your autoencoder\n",
    "decoded_imgs = autoencoder.predict(X_test_noise)\n",
    "\n",
    "# Plot noisy vs decoded images\n",
    "compare_plot(X_test_noise, decoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noise is gone now! You could get a better reconstruction by using a convolutional autoencoder. I hope this new model opened up your mind to the many possible architectures and non-classical ML problems that neural networks can solve :)\n",
    "\n",
    "1. Intro to CNNs\n",
    "\n",
    "Lets introduce Convolutional Neural Networks, a specific type of network that has led to a lot of advances in computer vision, as well as in other areas.\n",
    "2. How do they work?\n",
    "\n",
    "A convolutional model uses convolutional layers. A convolution is a simple mathematical operation that preserves spatial relationships. When applied to images it can detect relevant areas of interest like edges, corners, vertical lines, etc.\n",
    "3. Convolutions demonstration\n",
    "\n",
    "It consists of applying a filter also known as kernel of a given size. In this image, we are applying a 3 by 3 kernel. We center the kernel matrix of numbers as we slide through each pixel in the image, multiplying the kernel and pixel values at each location and averaging the sum of values obtained. This effectively computes a new image where certain characteristics are amplified depending on the filter used. The secret sauce of CNNs resides in letting the network itself find the best filter values and to combine them to achieve a given task.\n",
    "4. Typical architectures\n",
    "\n",
    "For a classification problem with many possible classes, CNNs tend to become very deep. Architectures consist of concatenations of convolutional layers among other layers known as pooling layers, that we won't cover here. Convolutional layers perform feature learning, we then flatten the outputs into a unidimensional vector and pass it to fully connected layers that carry out classification.\n",
    "5. Input shape to convolutional neural networks\n",
    "\n",
    "Images are 3D tensors, they have width, height, and depth. This depth is given by the color channels. If we use black and white images we will just have one channel, so the depth will be 1.\n",
    "6. How to build a simple convolutional net in keras?\n",
    "\n",
    "To build a CNN in Keras we first import the Conv2D and Flatten layers from keras.layers. We instantiate our model and add a convolutional layer. This first convolutional layer has 32 filters, this means it will learn 32 different convolutional masks. These masks will be squares of 3 by 3 as defined in the kernel_size. For 28 times 28 black and white images with only one channel, we use an input shape of (28, 28, 1). We can use any activation, as usual. We then add another convolutional layer and end flattening this 2D layer into a unidimensional layer with the Flatten layer. We finish with an output dense layer.\n",
    "7. Deep convolutional models\n",
    "\n",
    "ResNet50 is a 50 layer-deep model that performs well on the Imagenet Dataset, a huge dataset of more than 14 million images. ResNet50 can distinguish between 1000 different classes. This model would take too long to train on a regular computer, but Keras makes it easy for us to use it. We just need to prepare the image we want to classify for the model, predict the processed image, and decode the predictions!\n",
    "8. Pre-processing images for ResNet50\n",
    "\n",
    "To use pre-trained models to classify images, we first have to adapt these images so that they can be understood by the model. To prepare images for ResNet50 we would do the following. First import the image from keras.preprocessing and preprocess_input from keras.applications.resnet50. We then load our image with load_img, providing the target size, for this particular model that is 224 by 224. We turn the image into a numpy array with img_to_array, we expand the dimensions of the array and preprocess the input in the same way the training images were.\n",
    "9. Using the ResNet50 model in Keras\n",
    "\n",
    "We import ResNet50 and decode_predictions,load the model with Imagenet pre_trained weights,predict on our image,and decode the predictions. That is, getting the predicted classes with the highest probabilities.\n",
    "10. What is going on inside a convnet?\n",
    "\n",
    "Inside a CNN we can check how the different filters activate in response to an input image. We will explore this in the exercises!\n",
    "# Building a CNN model\n",
    "\n",
    "Building a CNN model in Keras isn't much more difficult than building any of the models you've already built throughout the course! You just need to make use of convolutional layers.\n",
    "\n",
    "You're going to build a shallow convolutional model that classifies the MNIST digits dataset. The same one you de-noised with your autoencoder! The images are 28 x 28 pixels and just have one channel, since they are black and white pictures.\n",
    "\n",
    "Go ahead and build this small convolutional model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Conv2D and Flatten layers and instantiate model\n",
    "from keras.layers import Conv2D,Flatten\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer of 32 filters of size 3x3 \n",
    "model.add(Conv2D(32, kernel_size = 3, input_shape = (28, 28, 1), activation = 'relu'))\n",
    "\n",
    "# Add a convolutional layer of 16 filters of size 3x3\n",
    "model.add(Conv2D(16, kernel_size = 3, activation = 'relu'))\n",
    "\n",
    "# Flatten the previous layer output \n",
    "model.add(Flatten())\n",
    "\n",
    "# Add as many outputs as classes with softmax activation\n",
    "model.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at convolutions\n",
    "\n",
    "Inspecting the activations of a convolutional layer is a cool thing. You have to do it at least once in your lifetime!\n",
    "\n",
    "To do so, you will build a new model with the Keras Model object, which takes in a list of inputs and a list of outputs. The output you will provide to this new model is the first convolutional layer outputs when given an MNIST digit as input image.\n",
    "\n",
    "The convolutional model you built in the previous exercise has already been trained for you. It can now correctly classify MNIST handwritten images. You can check it with model.summary() in the console.\n",
    "\n",
    "Let's look at the convolutional masks that were learned in the first convolutional layer of this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a reference to the outputs of the first layer\n",
    "first_layer_output = model.layers[0].output\n",
    "\n",
    "# Build a model using the model input and the first layer output\n",
    "first_layer_model = Model(inputs = model.layers[0].input, outputs = first_layer_output)\n",
    "\n",
    "# Use this model to predict on X_test\n",
    "activations = first_layer_model.predict(X_test)\n",
    "\n",
    "# Plot the first digit of X_test for the 15th filter\n",
    "axs[0].matshow(activations[0,:,:,14], cmap = 'viridis')\n",
    "\n",
    "# Do the same but for the 18th filter now\n",
    "axs[1].matshow(activations[0,:,:,17], cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing your input image\n",
    "\n",
    "The original ResNet50 model was trained with images of size 224 x 224 pixels and a number of preprocessing operations; like the subtraction of the mean pixel value in the training set for all training images. You need to pre-process the images you want to predict on in the same way.\n",
    "\n",
    "When predicting on a single image you need it to fit the model's input shape, which in this case looks like this: (batch-size, width, height, channels),np.expand_dims with parameter axis = 0 adds the batch-size dimension, representing that a single image will be passed to predict. This batch-size dimension value is 1, since we are only predicting on one image.\n",
    "\n",
    "You will go over these preprocessing steps as you prepare this dog's (named Ivy) image into one that can be classified by ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image and preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size = (224, 224))\n",
    "\n",
    "# Turn it into an array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Expand the dimensions of the image, this is so that it fits the expected model input format\n",
    "img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "# Pre-process the img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a ResNet50 model with 'imagenet' weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on your already processed img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# Decode the first 3 predictions\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LSTMs\n",
    "\n",
    "It's time to briefly introduce Long Short Term Memory networks,also known as LSTMs.\n",
    "2. What are RNNs?\n",
    "\n",
    "LSTMs are a type of recurrent neural network, RNN for short. A simple RNN is a neural network that can use past predictions in order to infer new ones. This allows us to solve problems where there is a dependence on past inputs.\n",
    "3. What are LSTMs?\n",
    "\n",
    "LSTM neurons are pretty complex, they are actually called units or cells. They have an internal state that is passed between units, you can see this as a memory of past steps. A unit receives the internal state, an output from the previous unit, and a new input at time t. Then it updates the state and produces a new output that is returned, as well as passed as an input to the following unit.\n",
    "4. What are LSTMs?\n",
    "\n",
    "LSTM units perform several operations. They learn what to ignore, what to keep and to select the most important pieces of past information in order to predict the future. They tend to work better than simple RNNs for most problems.\n",
    "5. When to use LSTMs?\n",
    "\n",
    "LSTMs have been used for image captioning, speech to text, text translation, document summarization, text generation, musical composition,and many more.\n",
    "\n",
    "    1 Karpathy, A., & Fei-Fei, L. (2015). Deep visual-semantic alignments for generating image descriptions.\n",
    "\n",
    "6. LSTMs + Text\n",
    "\n",
    "Let's go over an example on how to use LSTMs with text data to predict the next word in a sentence!\n",
    "7. Embeddings\n",
    "\n",
    "Neural networks can only deal with numbers, not text. We need to transform each unique word into a number. Then these numbers can be used as inputs to an embedding layer.\n",
    "8. Embeddings\n",
    "\n",
    "Embedding layers learn to represent words as vectors of a predetermined size. These vectors encode meaning and are used by subsequent layers.\n",
    "9. Sequence preparation\n",
    "\n",
    "We first define some text and choose a sequence length. With a sequence length of 3 we will end up feeding our model with two words and it will predict the third one. We split the text into words with the split method. The output looks like this: We then need to turn these words into consecutive lines of 3 words each. We can loop from seq_len to the number of words + 1 and store each line. The end results look like this:\n",
    "10. Text preparation in Keras\n",
    "\n",
    "After that we turn our text sequences into numbers. We import Keras Tokenizer from the preprocessing text module. Instantiate it, fit it on lines,and then turn those lines into numeric sequences. This is how the 3-word lines look now. The tokenizer object stores the word-to-number mapping. There are two dictionaries, the index_word, and the word_index. Here, the index_word is printed, which shows the encoded word for each index. We can use this dictionary to decode our outputs, mapping numbers to words.\n",
    "11. Building a LSTM model\n",
    "\n",
    "Our data is ready to be processed. Now, we are ready to build the LSTM model. We start by importing the Dense, LSTM, and Embedding layers from Keras layers. We then store the vocab_size, since we will use it when defining our layers. The vocab_size is the length of the tokenizer dictionary plus one. The plus one is because we account for 0 as an integer reserved for special characters, as we saw, our dictionary starts at 1, not 0. We add an embedding layer, the input_dim is the vocab_size variable, we will turn our word numbers into 8-dimensional vectors, and need to declare the input_length so that our model understand that two words will be passed simultaneously as a sequence. We end by adding an LSTM layer of 8 units, a hidden layer, and an output layer with softmax and as many outputs as possible words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text prediction with LSTMs\n",
    "\n",
    "During the following exercises you will build a toy LSTM model that is able to predict the next word using a small text dataset. This dataset consist of cleaned quotes from the The Lord of the Ring movies. You can find them in the text variable.\n",
    "\n",
    "You will turn this text into sequences of length 4 and make use of the Keras Tokenizer to prepare the features and labels for your model!\n",
    "\n",
    "The Keras Tokenizer is already imported for you to use. It assigns a unique number to each unique word, and stores the mappings in a dictionary. This is important since the model deals with numbers but we later will want to decode the output numbers back into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into an array of words\n",
    "words = text.split()\n",
    "\n",
    "# Make sentences of 4 words each, moving one word at a time\n",
    "sentences = []\n",
    "for i in range(4, len(words)):\n",
    "    sentences.append(' '.join(words[i-4:i]))\n",
    "  \n",
    "#Instantiate a Tokenizer, then fit it on the sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turn sentences into a sequence of numbers\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"Sentences: \\n {} \\n Sequences: \\n {}\".format(sentences[:5],sequences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your LSTM model\n",
    "\n",
    "You've already prepared your sequences of text. It's time to build your LSTM model!\n",
    "\n",
    "Remember your sequences had 4 words each, your model will be trained on the first three words of each sequence, predicting the 4th one. You are going to use an Embedding layer that will essentially learn to turn words into vectors. These vectors will then be passed to a simple LSTM layer. Our output is a Dense layer with as many neurons as words in the vocabulary and softmax activation. This is because we want to obtain the highest probable next word out of all possible words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Embedding, LSTM and Dense layer\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer with the right parameters\n",
    "model.add(Embedding(input_dim =vocab_size, input_length = 3, output_dim =8, ))\n",
    "\n",
    "# Add a 32 unit LSTM layer\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Add a hidden Dense layer of 32 units and an output layer of vocab_size with softmax\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a nice looking model you've built! You'll see that this model is powerful enough to learn text relationships, we aren't using a lot of text in this tiny example and our sequences are quite short. This model is to be trained as usual, you would just need to compile it with an optimizer like adam and use crossentropy loss. This is because we have modeled this next word prediction task as a classification problem with all the unique words in our vocabulary as candidate classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(test_text, model = model):\n",
    "    if len(test_text.split()) != 3:\n",
    "    print('Text input should be 3 words!')\n",
    "    return False\n",
    "\n",
    "    # Turn the test_text into a sequence of numbers\n",
    "    test_seq = tokenizer.texts_to_sequences([test_text])\n",
    "    test_seq = np.array(test_seq)\n",
    "\n",
    "    # Use the model passed as a parameter to predict the next word\n",
    "    pred = model.predict(test_seq).argmax(axis = 1)[0]\n",
    "\n",
    "    # Return the word that maps to the prediction\n",
    "    return tokenizer.index_word[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
